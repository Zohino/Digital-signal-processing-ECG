{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d545dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG Analysis Pipeline: R-peak Detection and Anomaly Detection\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import find_peaks, butter, sosfiltfilt\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Download datasets if needed (commented out to avoid re-downloading)\n",
    "# wfdb.dl_database(\"butqdb\", \"source1\", keep_subdirs=False)\n",
    "# wfdb.dl_database(\"nsrdb\", \"source2\", keep_subdirs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96467c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. SIGNAL PREPROCESSING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def apply_highpass_filter(signal: np.ndarray, fs: int, low: float = 0.5, high: float = 45.0, order: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply Butterworth high-pass filter to remove baseline drift.\n",
    "\n",
    "    Args:\n",
    "        signal: Input ECG signal array\n",
    "        fs: Sampling frequency in Hz\n",
    "        cutoff: High-pass cutoff frequency in Hz (default: 0.5)\n",
    "        order: Filter order (default: 4)\n",
    "\n",
    "    Returns:\n",
    "        Filtered signal with baseline drift removed\n",
    "    \"\"\"\n",
    "    sos = butter(order, [low / (0.5 * fs), high / (0.5 * fs)], btype='bandpass', output='sos')\n",
    "    return sosfiltfilt(sos, signal)\n",
    "\n",
    "def normalize_signal(signal: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize signal to zero mean and unit variance.\n",
    "\n",
    "    Args:\n",
    "        signal: Input signal array\n",
    "\n",
    "    Returns:\n",
    "        Normalized signal array\n",
    "    \"\"\"\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "def preprocess_ecg_signal(signal: np.ndarray, fs: int, normalize: bool = True, order: int = 4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Complete ECG preprocessing pipeline including DC removal and filtering.\n",
    "\n",
    "    Args:\n",
    "        signal: Raw ECG signal array\n",
    "        fs: Sampling frequency in Hz\n",
    "        normalize: Whether to normalize the signal (default: True)\n",
    "        cutoff: High-pass cutoff frequency in Hz (default: 0.7)\n",
    "        order: Filter order (default: 4)\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed ECG signal ready for analysis\n",
    "    \"\"\"\n",
    "    signal = apply_highpass_filter(signal, fs, order=order)\n",
    "    if normalize:\n",
    "        signal = normalize_signal(signal)\n",
    "    return signal\n",
    "\n",
    "class ECGStreamProcessor:\n",
    "    \"\"\"\n",
    "    ECG stream processor for chunked reading to manage memory usage with large files.\n",
    "\n",
    "    This class processes long ECG recordings by reading them in chunks rather than\n",
    "    loading the entire file into memory at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000, chunk_duration_sec: int = 600):\n",
    "        \"\"\"\n",
    "        Initialize the ECG stream processor.\n",
    "\n",
    "        Args:\n",
    "            fs: Sampling frequency in Hz (default: 1000)\n",
    "            chunk_duration_sec: Duration of each processing chunk in seconds (default: 600, 10 minutes)\n",
    "        \"\"\"\n",
    "        self.fs = fs\n",
    "        self.chunk_size = chunk_duration_sec * fs\n",
    "        self.overlap_size = int(fs * 5.0)  # 5 second overlap for continuity\n",
    "\n",
    "    def process_file_in_chunks(self, record_path: str, processor_func,\n",
    "                              max_duration_hours: float = None) -> List:\n",
    "        \"\"\"\n",
    "        Process ECG file in chunks to manage memory usage.\n",
    "\n",
    "        Args:\n",
    "            record_path: Path to ECG record file\n",
    "            processor_func: Function to apply to each chunk (chunk, chunk_start) -> result\n",
    "            max_duration_hours: Maximum duration to process in hours (None for full file)\n",
    "\n",
    "        Returns:\n",
    "            List of results from processing each chunk\n",
    "        \"\"\"\n",
    "        # Get file information\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * self.fs)\n",
    "            total_samples = min(total_samples, max_samples)\n",
    "\n",
    "        results = []\n",
    "        processed_samples = 0\n",
    "\n",
    "        print(f\"Processing {total_samples/self.fs/3600:.2f} hours in chunks of {self.chunk_size/self.fs/60:.1f} minutes\")\n",
    "\n",
    "        # Process file in chunks\n",
    "        with tqdm(total=total_samples, desc=\"Processing chunks\") as pbar:\n",
    "            while processed_samples < total_samples:\n",
    "                # Calculate chunk boundaries\n",
    "                start_sample = max(0, processed_samples - self.overlap_size)\n",
    "                end_sample = min(total_samples, processed_samples + self.chunk_size)\n",
    "\n",
    "                # Read chunk from file\n",
    "                record = wfdb.rdrecord(record_path, sampfrom=start_sample, sampto=end_sample)\n",
    "                chunk = record.p_signal[:, 0]\n",
    "\n",
    "                # Preprocess chunk\n",
    "                chunk_processed = preprocess_ecg_signal(chunk, self.fs)\n",
    "\n",
    "                # Apply processor function\n",
    "                processor_func(chunk_processed, start_sample)\n",
    "\n",
    "                # Update progress\n",
    "                processed_samples += self.chunk_size\n",
    "                pbar.update(min(self.chunk_size, end_sample - (processed_samples - self.chunk_size)))\n",
    "\n",
    "                # Force garbage collection to manage memory\n",
    "                del chunk, chunk_processed, record\n",
    "                gc.collect()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61aedbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. R-PEAK DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_r_peaks_threshold(signal: np.ndarray, fs: int, height_percentile: float = 87,\n",
    "                            min_distance_ms: float = 250) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect R-peaks using simple peak detection with dynamic threshold.\n",
    "\n",
    "    Args:\n",
    "        signal: Preprocessed ECG signal\n",
    "        fs: Sampling frequency in Hz\n",
    "        height_percentile: Percentile for dynamic threshold calculation (default: 85)\n",
    "        min_distance_ms: Minimum distance between peaks in milliseconds assuming 240 BPM max (default: 250)\n",
    "\n",
    "    Returns:\n",
    "        Array of R-peak indices in the signal\n",
    "    \"\"\"\n",
    "    height = np.percentile(signal, height_percentile)\n",
    "    height_neg = np.percentile(-signal, height_percentile)\n",
    "    min_distance = int(min_distance_ms * fs / 1000)\n",
    "    peaks_pos, _ = find_peaks(signal, height=height, distance=min_distance)\n",
    "    peaks_neg, _ = find_peaks(-signal, height=height_neg, distance=min_distance)\n",
    "    if len(peaks_neg) > len(peaks_pos):\n",
    "        peaks = peaks_neg\n",
    "    else:\n",
    "        peaks = peaks_pos\n",
    "    return peaks\n",
    "\n",
    "class RPeakDetector:\n",
    "    \"\"\"\n",
    "    R-peak detector for long ECG signals using chunked processing.\n",
    "\n",
    "    This class accumulates R-peak detections across multiple chunks while\n",
    "    avoiding duplicate detections in overlap regions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize R-peak detector.\n",
    "\n",
    "        Args:\n",
    "            fs: Sampling frequency in Hz (default: 1000)\n",
    "        \"\"\"\n",
    "        self.fs = fs\n",
    "        self.all_peaks = []\n",
    "        self.overlap_samples = int(fs * 5.0)  # 5 second overlap\n",
    "\n",
    "    def process_chunk_for_peaks(self, chunk: np.ndarray, chunk_start: int) -> None:\n",
    "        \"\"\"\n",
    "        Process a single chunk for R-peak detection.\n",
    "\n",
    "        Args:\n",
    "            chunk: ECG signal chunk\n",
    "            chunk_start: Starting sample index of this chunk in the full signal\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with processing statistics for this chunk\n",
    "        \"\"\"\n",
    "        # Detect peaks in current chunk\n",
    "        peaks = detect_r_peaks_threshold(chunk, self.fs)\n",
    "\n",
    "        if len(peaks) == 0:\n",
    "            return\n",
    "\n",
    "        # Adjust peak indices to global position\n",
    "        global_peaks = peaks + chunk_start\n",
    "\n",
    "        # Store peaks while avoiding overlap region duplicates\n",
    "        if chunk_start > 0:\n",
    "            # Skip overlap region for non-first chunks\n",
    "            valid_mask = peaks >= self.overlap_samples\n",
    "            global_peaks = global_peaks[valid_mask]\n",
    "\n",
    "        self.all_peaks.extend(global_peaks.tolist())\n",
    "\n",
    "        del peaks, global_peaks\n",
    "        gc.collect()\n",
    "\n",
    "    def get_all_detected_peaks(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get all detected R-peaks from all processed chunks.\n",
    "\n",
    "        Returns:\n",
    "            Sorted array of all R-peak sample indices\n",
    "        \"\"\"\n",
    "        if not self.all_peaks:\n",
    "            return np.array([])\n",
    "        return np.array(sorted(self.all_peaks))\n",
    "\n",
    "    def clear_peaks(self) -> None:\n",
    "        \"\"\"Clear stored peaks to free memory.\"\"\"\n",
    "        self.all_peaks.clear()\n",
    "        gc.collect()\n",
    "\n",
    "def calculate_bpm_from_peaks(rpeaks: np.ndarray, fs: int,\n",
    "                            total_duration_sec: float, window_sec: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate BPM (beats per minute) in sliding windows from R-peak locations.\n",
    "\n",
    "    Args:\n",
    "        rpeaks: Array of R-peak sample indices\n",
    "        fs: Sampling frequency in Hz\n",
    "        total_duration_sec: Total signal duration in seconds\n",
    "        window_sec: Window size for BPM calculation in seconds (default: 60)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (bpm_values, time_minutes) arrays\n",
    "    \"\"\"\n",
    "    if len(rpeaks) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    n_windows = int(total_duration_sec // window_sec)\n",
    "    bpm_values = np.zeros(n_windows)\n",
    "\n",
    "    # Use binary search for efficient counting\n",
    "    window_starts = np.arange(n_windows) * window_sec * fs\n",
    "    window_ends = window_starts + window_sec * fs\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        start_idx = np.searchsorted(rpeaks, window_starts[i], side='left')\n",
    "        end_idx = np.searchsorted(rpeaks, window_ends[i], side='right')\n",
    "        peaks_in_window = end_idx - start_idx\n",
    "        bpm_values[i] = peaks_in_window * 60 / window_sec\n",
    "\n",
    "    time_minutes = np.arange(n_windows) * (window_sec / 60)\n",
    "    return bpm_values, time_minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29bd87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SINGLE FILE ECG PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_single_ecg_file(data_dir: str, record_name: str, plot: bool = True,\n",
    "                           max_duration_hours: float = 1.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a single ECG file for R-peak detection and BPM calculation with memory optimization.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files\n",
    "        record_name: Name of the ECG record (without file extension)\n",
    "        plot: Whether to plot the BPM over time (default: True)\n",
    "        max_duration_hours: Maximum duration to process in hours (default: 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing analysis results and statistics including:\n",
    "        - record_name: Name of processed record\n",
    "        - success: Boolean indicating if processing was successful\n",
    "        - fs: Sampling frequency\n",
    "        - total_samples: Number of samples processed\n",
    "        - detected_rpeaks: Number of R-peaks detected\n",
    "        - average_bpm: Average BPM across the signal\n",
    "        - bpm_values: Array of BPM values over time\n",
    "        - time_minutes: Array of time points in minutes\n",
    "        - processing_duration_hours: Duration of processed signal\n",
    "        - error: Error message if processing failed\n",
    "    \"\"\"\n",
    "    record_path = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "    results = {\n",
    "        'record_name': record_name,\n",
    "        'success': False,\n",
    "        'fs': None,\n",
    "        'total_samples': None,\n",
    "        'detected_rpeaks': None,\n",
    "        'average_bpm': None,\n",
    "        'bpm_values': None,\n",
    "        'time_minutes': None,\n",
    "        'processing_duration_hours': None,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get file information\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        fs = header.fs\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        processing_duration_hours = processing_samples / fs / 3600\n",
    "\n",
    "        print(f\"  Total file duration: {total_samples/fs/3600:.2f} hours\")\n",
    "        print(f\"  Processing duration: {processing_duration_hours:.2f} hours\")\n",
    "\n",
    "        # Initialize stream processor and R-peak detector\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=600)  # 10-minute chunks\n",
    "        detector = RPeakDetector(fs=fs)\n",
    "\n",
    "        # Process file in chunks\n",
    "        processor.process_file_in_chunks(record_path, detector.process_chunk_for_peaks, max_duration_hours)\n",
    "\n",
    "        # Get all detected R-peaks\n",
    "        rpeaks = detector.get_all_detected_peaks()\n",
    "\n",
    "        # Compute BPM over time\n",
    "        total_duration_sec = processing_samples / fs\n",
    "        bpm_values, time_minutes = calculate_bpm_from_peaks(\n",
    "            rpeaks, fs, total_duration_sec\n",
    "        )\n",
    "        avg_bpm = np.mean(bpm_values) if len(bpm_values) > 0 else 0\n",
    "\n",
    "        # Update results\n",
    "        results.update({\n",
    "            'fs': fs,\n",
    "            'total_samples': processing_samples,\n",
    "            'detected_rpeaks': len(rpeaks),\n",
    "            'average_bpm': avg_bpm,\n",
    "            'bpm_values': bpm_values,\n",
    "            'time_minutes': time_minutes,\n",
    "            'processing_duration_hours': processing_duration_hours,\n",
    "            'success': True\n",
    "        })\n",
    "\n",
    "        # Plot results if requested\n",
    "        if plot and len(bpm_values) > 0:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(time_minutes, bpm_values, \"-\", linewidth=1, alpha=0.8)\n",
    "            plt.title(f\"BPM Over Time - {record_name} ({processing_duration_hours:.1f}h)\")\n",
    "            plt.xlabel(\"Time (minutes)\")\n",
    "            plt.ylabel(\"BPM\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"Record: {record_name}\")\n",
    "        print(f\"Sampling rate: {fs} Hz\")\n",
    "        print(f\"Processed samples: {processing_samples:,}\")\n",
    "        print(f\"Detected R-peaks: {len(rpeaks):,}\")\n",
    "        print(f\"Average BPM: {avg_bpm:.1f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        detector.clear_peaks()\n",
    "\n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {record_name}: {str(e)}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4fc3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. BATCH ECG PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_ecg_batch(data_dir: str, plot_individual: bool = False,\n",
    "                     max_duration_hours: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Batch process multiple ECG files in a directory with memory optimization.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files\n",
    "        plot_individual: Whether to plot individual file results (default: False)\n",
    "        max_duration_hours: Maximum duration to process per file in hours (default: 1.0)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing summary statistics for all processed files including:\n",
    "        - Record: Record name\n",
    "        - Sampling_Rate_Hz: Sampling frequency\n",
    "        - Total_Samples: Number of samples processed\n",
    "        - Detected_R_peaks: Number of R-peaks detected\n",
    "        - Average_BPM: Average BPM for the record\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_files = os.listdir(data_dir)\n",
    "    ecg_files = [f for f in all_files if f.endswith('_ECG.hea')]\n",
    "    record_names = sorted([f.replace('_ECG.hea', '') for f in ecg_files])\n",
    "\n",
    "    print(f\"Processing {len(record_names)} ECG records from {data_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(record_names)}: {record_name}\")\n",
    "\n",
    "        result = analyze_single_ecg_file(\n",
    "            data_dir, record_name, plot=plot_individual,\n",
    "            max_duration_hours=max_duration_hours\n",
    "        )\n",
    "\n",
    "        if result.get('success', False):\n",
    "            results.append({\n",
    "                'Record': result['record_name'],\n",
    "                'Sampling_Rate_Hz': result['fs'],\n",
    "                'Total_Samples': result['total_samples'],\n",
    "                'Detected_R_peaks': result['detected_rpeaks'],\n",
    "                'Average_BPM': result['average_bpm']\n",
    "            })\n",
    "\n",
    "        # Force garbage collection between files\n",
    "        del result\n",
    "        gc.collect()\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nBatch Processing Summary:\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d046f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Anomaly detector for ECG signals using multiple detection methods.\n",
    "\n",
    "    This class applies various anomaly detection techniques and tracks\n",
    "    where in the signal anomalies are detected for visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize anomaly detector.\n",
    "\n",
    "        Args:\n",
    "            fs: Sampling frequency in Hz (default: 1000)\n",
    "        \"\"\"\n",
    "        self.fs = fs\n",
    "        self.all_anomalies = []\n",
    "        self.overlap_samples = int(fs * 5.0)  # 5 second overlap\n",
    "\n",
    "    def detect_amplitude_anomalies(self, signal: np.ndarray, k: float = 3.5) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Detect anomalies based on amplitude threshold using standard deviation.\n",
    "\n",
    "        Args:\n",
    "            signal: ECG signal chunk\n",
    "            k: Number of standard deviations for threshold (default: 3.5)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (anomaly_indices, threshold_value)\n",
    "        \"\"\"\n",
    "        mean_val = signal.mean()\n",
    "        std_val = signal.std()\n",
    "        threshold = mean_val + k * std_val\n",
    "        anomaly_indices = np.where(np.abs(signal) > threshold)[0]\n",
    "        return anomaly_indices, threshold\n",
    "\n",
    "    def detect_rolling_statistics_anomalies(self, signal: np.ndarray, window: int = 1000, threshold: float = 5.5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Detect anomalies using rolling mean and standard deviation.\n",
    "\n",
    "        Args:\n",
    "            signal: ECG signal chunk\n",
    "            window: Rolling window size in samples (default: 1000)\n",
    "            threshold: Number of rolling standard deviations for anomaly threshold (default: 5.5)\n",
    "\n",
    "        Returns:\n",
    "            Array of anomaly indices\n",
    "        \"\"\"\n",
    "        if len(signal) < window:\n",
    "            window = len(signal) // 2\n",
    "\n",
    "        rolling_mean = np.convolve(signal, np.ones(window) / window, mode='same')\n",
    "        rolling_std = np.sqrt(np.convolve((signal - rolling_mean)**2, np.ones(window) / window, mode='same'))\n",
    "\n",
    "        # Avoid division by zero\n",
    "        rolling_std = np.maximum(rolling_std, 1e-8)\n",
    "        anomaly_indices = np.where(np.abs(signal - rolling_mean) > threshold * rolling_std)[0]\n",
    "        return anomaly_indices\n",
    "\n",
    "    def detect_iqr_anomalies(self, signal: np.ndarray, k: float = 5.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Detect anomalies using Interquartile Range (IQR) method.\n",
    "\n",
    "        Args:\n",
    "            signal: ECG signal chunk\n",
    "            k: IQR multiplier for outlier detection (default: 5.0)\n",
    "\n",
    "        Returns:\n",
    "            Array of anomaly indices\n",
    "        \"\"\"\n",
    "        q1, q3 = np.percentile(signal, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:  # Handle constant signal\n",
    "            return np.array([])\n",
    "        lower_bound = q1 - k * iqr\n",
    "        upper_bound = q3 + k * iqr\n",
    "        anomaly_indices = np.where((signal < lower_bound) | (signal > upper_bound))[0]\n",
    "        return anomaly_indices\n",
    "\n",
    "    def process_chunk_for_anomalies(self, chunk: np.ndarray, chunk_start: int) -> None:\n",
    "        \"\"\"\n",
    "        Process a single chunk for anomaly detection using multiple methods.\n",
    "\n",
    "        Args:\n",
    "            chunk: ECG signal chunk\n",
    "            chunk_start: Starting sample index of this chunk in the full signal\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with anomaly detection statistics for this chunk\n",
    "        \"\"\"\n",
    "        # Apply multiple detection methods\n",
    "        methods_results = []\n",
    "\n",
    "        # Amplitude-based detection\n",
    "        amp_anomalies, _ = self.detect_amplitude_anomalies(chunk)\n",
    "        methods_results.append(amp_anomalies)\n",
    "\n",
    "        # Rolling statistics detection\n",
    "        rolling_anomalies = self.detect_rolling_statistics_anomalies(chunk)\n",
    "        methods_results.append(rolling_anomalies)\n",
    "\n",
    "        # IQR-based detection\n",
    "        iqr_anomalies = self.detect_iqr_anomalies(chunk)\n",
    "        methods_results.append(iqr_anomalies)\n",
    "\n",
    "        # Combine results from all methods\n",
    "        if any(len(arr) > 0 for arr in methods_results):\n",
    "            combined_anomalies = np.unique(np.concatenate([arr for arr in methods_results if len(arr) > 0]))\n",
    "        else:\n",
    "            combined_anomalies = np.array([])\n",
    "\n",
    "        if len(combined_anomalies) == 0:\n",
    "            return\n",
    "\n",
    "        # Adjust indices to global position\n",
    "        global_anomalies = combined_anomalies + chunk_start\n",
    "\n",
    "        # Store anomalies while avoiding overlap region duplicates\n",
    "        if chunk_start > 0:\n",
    "            # Skip overlap region for non-first chunks\n",
    "            valid_mask = combined_anomalies >= self.overlap_samples\n",
    "            global_anomalies = global_anomalies[valid_mask]\n",
    "\n",
    "        self.all_anomalies.extend(global_anomalies.tolist())\n",
    "\n",
    "        del amp_anomalies, rolling_anomalies, iqr_anomalies, combined_anomalies, global_anomalies\n",
    "        gc.collect()\n",
    "\n",
    "    def get_all_detected_anomalies(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get all detected anomalies from all processed chunks.\n",
    "\n",
    "        Returns:\n",
    "            Sorted array of all anomaly sample indices\n",
    "        \"\"\"\n",
    "        if not self.all_anomalies:\n",
    "            return np.array([])\n",
    "        return np.array(sorted(self.all_anomalies))\n",
    "\n",
    "    def clear_anomalies(self) -> None:\n",
    "        \"\"\"Clear stored anomalies to free memory.\"\"\"\n",
    "        self.all_anomalies.clear()\n",
    "        gc.collect()\n",
    "\n",
    "def analyze_anomalies_in_file(data_dir: str, record_name: str,\n",
    "                             max_duration_hours: float = 1.0,\n",
    "                             plot_anomalies: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect and analyze anomalies in a single ECG file with location plotting.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files\n",
    "        record_name: Name of the ECG record (without file extension)\n",
    "        max_duration_hours: Maximum duration to process in hours (default: 1.0)\n",
    "        plot_anomalies: Whether to plot anomaly locations in the signal (default: True)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing anomaly detection results including:\n",
    "        - record_name: Name of processed record\n",
    "        - success: Boolean indicating if processing was successful\n",
    "        - detected_anomalies: Number of anomalies detected\n",
    "        - anomaly_rate_per_hour: Rate of anomalies per hour\n",
    "        - processing_duration_hours: Duration of processed signal\n",
    "        - anomaly_times_minutes: Array of anomaly times in minutes\n",
    "        - error: Error message if processing failed\n",
    "    \"\"\"\n",
    "    record_path = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "    results = {\n",
    "        'record_name': record_name,\n",
    "        'success': False,\n",
    "        'detected_anomalies': 0,\n",
    "        'anomaly_rate_per_hour': 0,\n",
    "        'processing_duration_hours': 0,\n",
    "        'anomaly_times_minutes': None,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get file information\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        fs = header.fs\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        processing_duration_hours = processing_samples / fs / 3600\n",
    "\n",
    "        # Initialize stream processor and anomaly detector\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=600)  # 10-minute chunks\n",
    "        detector = AnomalyDetector(fs=fs)\n",
    "\n",
    "        # Process file in chunks\n",
    "        processor.process_file_in_chunks(record_path, detector.process_chunk_for_anomalies, max_duration_hours)\n",
    "\n",
    "        # Get all detected anomalies\n",
    "        anomalies = detector.get_all_detected_anomalies()\n",
    "        anomaly_rate = len(anomalies) / processing_duration_hours if processing_duration_hours > 0 else 0\n",
    "\n",
    "        # Convert anomaly indices to time in minutes\n",
    "        anomaly_times_minutes = anomalies / fs / 60\n",
    "\n",
    "        results.update({\n",
    "            'success': True,\n",
    "            'detected_anomalies': len(anomalies),\n",
    "            'anomaly_rate_per_hour': anomaly_rate,\n",
    "            'processing_duration_hours': processing_duration_hours,\n",
    "            'anomaly_times_minutes': anomaly_times_minutes\n",
    "        })\n",
    "\n",
    "        # Plot anomaly locations if requested\n",
    "        if plot_anomalies and len(anomalies) > 0:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "\n",
    "            # Top subplot: Anomaly timeline\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.scatter(anomaly_times_minutes, np.ones(len(anomaly_times_minutes)),\n",
    "                       c='red', alpha=0.6, s=20)\n",
    "            plt.title(f\"Anomaly Locations in {record_name} ({len(anomalies)} anomalies detected)\")\n",
    "            plt.xlabel(\"Time (minutes)\")\n",
    "            plt.ylabel(\"Anomalies\")\n",
    "            plt.ylim(0.5, 1.5)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # Bottom subplot: Anomaly density histogram\n",
    "            plt.subplot(2, 1, 2)\n",
    "            if len(anomaly_times_minutes) > 1:\n",
    "                bins = min(50, len(anomaly_times_minutes) // 2 + 1)\n",
    "                plt.hist(anomaly_times_minutes, bins=bins, alpha=0.7, color='red', edgecolor='black')\n",
    "                plt.title(\"Anomaly Density Distribution\")\n",
    "                plt.xlabel(\"Time (minutes)\")\n",
    "                plt.ylabel(\"Number of Anomalies\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"Insufficient anomalies for density plot\",\n",
    "                        transform=plt.gca().transAxes, ha='center', va='center')\n",
    "                plt.title(\"Anomaly Density Distribution\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        print(f\"Record: {record_name}\")\n",
    "        print(f\"Detected anomalies: {len(anomalies):,}\")\n",
    "        print(f\"Anomaly rate: {anomaly_rate:.1f} per hour\")\n",
    "        if len(anomalies) > 0:\n",
    "            print(f\"First anomaly at: {anomaly_times_minutes[0]:.2f} minutes\")\n",
    "            print(f\"Last anomaly at: {anomaly_times_minutes[-1]:.2f} minutes\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        detector.clear_anomalies()\n",
    "\n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {record_name}: {str(e)}\")\n",
    "    finally:\n",
    "        gc.collect\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "211aae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. R-PEAK EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_peak_detection_accuracy(true_peaks: np.ndarray, detected_peaks: np.ndarray, fs: int = 1000,\n",
    "                                   tolerance_ms: int = 100) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Evaluate R-peak detection accuracy by comparing with ground truth annotations.\n",
    "\n",
    "    Args:\n",
    "        true_peaks: Ground truth R-peak indices\n",
    "        detected_peaks: Algorithm-detected R-peak indices\n",
    "        fs: Sampling frequency in Hz (default: 1000)\n",
    "        tolerance_ms: Relative tolerance for matching peaks in miliseconds(default: 100)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (correct_detections, total_true_peaks, total_detected_peaks)\n",
    "    \"\"\"\n",
    "    tolerance_samples = int((tolerance_ms / 1000) * fs)\n",
    "\n",
    "    if len(true_peaks) == 0 or len(detected_peaks) == 0:\n",
    "        return 0, len(true_peaks), len(detected_peaks)\n",
    "\n",
    "    true_peaks = np.asarray(true_peaks)\n",
    "    detected_peaks = np.asarray(detected_peaks)\n",
    "\n",
    "    correct = 0\n",
    "    used = np.zeros(len(detected_peaks), dtype=bool)\n",
    "\n",
    "    for tp in true_peaks:\n",
    "        # Find detected peak within ±tolerance\n",
    "        idx = np.where((detected_peaks >= tp - tolerance_samples) &\n",
    "                       (detected_peaks <= tp + tolerance_samples) &\n",
    "                       (~used))[0]\n",
    "        if len(idx) > 0:\n",
    "            correct += 1\n",
    "            used[idx[0]] = True\n",
    "\n",
    "    return correct, len(true_peaks), len(detected_peaks)\n",
    "\n",
    "def evaluate_single_file_rpeak_detection(test_dir: str, base_name: str, fs: int = 1000,\n",
    "                                        max_duration_hours: float = 1.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate R-peak detection performance on a single file with ground truth.\n",
    "\n",
    "    Args:\n",
    "        test_dir: Directory containing test files with annotations\n",
    "        base_name: Base name of the file (without extension)\n",
    "        fs: Sampling frequency in Hz (default: 1000)\n",
    "        max_duration_hours: Maximum duration to process in hours (default: 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics including:\n",
    "        - File: Base filename\n",
    "        - Correct: Number of correctly detected peaks\n",
    "        - Missed: Number of missed peaks\n",
    "        - Total_True: Total number of true peaks\n",
    "        - Total_Detected: Total number of detected peaks\n",
    "        - Accuracy_Percent: Detection accuracy percentage\n",
    "        - Error: Error message if processing failed\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(test_dir, base_name)\n",
    "    result = {'File': base_name}\n",
    "\n",
    "    try:\n",
    "        # Get file information\n",
    "        header = wfdb.rdheader(file_path)\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        # Load ground truth annotations\n",
    "        annotations = wfdb.rdann(file_path, 'atr')\n",
    "        true_peaks = annotations.sample\n",
    "        true_peaks = true_peaks[true_peaks < processing_samples]\n",
    "\n",
    "        del annotations\n",
    "\n",
    "        # Process ECG in chunks for R-peak detection\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=600)\n",
    "        detector = RPeakDetector(fs=fs)\n",
    "\n",
    "        processor.process_file_in_chunks(file_path, detector.process_chunk_for_peaks, max_duration_hours)\n",
    "\n",
    "        detected_peaks = detector.get_all_detected_peaks()\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        correct, total_true, total_detected = evaluate_peak_detection_accuracy(true_peaks, detected_peaks)\n",
    "        accuracy = (correct / total_true * 100) if total_true > 0 else 0\n",
    "\n",
    "        result.update({\n",
    "            'Correct': correct,\n",
    "            'Missed': total_true - correct,\n",
    "            'Total_True': total_true,\n",
    "            'Total_Detected': total_detected,\n",
    "            'Accuracy_Percent': round(accuracy, 2)\n",
    "        })\n",
    "\n",
    "        del true_peaks, detected_peaks, processor, detector\n",
    "\n",
    "    except Exception as e:\n",
    "        result['Error'] = str(e)\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "    return result\n",
    "\n",
    "def evaluate_rpeak_detection_batch(test_dir: str, fs: int = 1000,\n",
    "                                  max_duration_hours: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate R-peak detection performance on multiple ECG files with ground truth annotations.\n",
    "\n",
    "    Processes files sequentially with aggressive memory management to prevent overflow.\n",
    "    Each file is processed independently with immediate cleanup to minimize memory footprint.\n",
    "\n",
    "    Args:\n",
    "        test_dir (str): Directory path containing test files with annotations (.dat files)\n",
    "        fs (int, optional): Sampling frequency in Hz. Defaults to 1000.\n",
    "        max_duration_hours (Optional[float], optional): Maximum duration to process per file in hours.\n",
    "                                                       If None, processes entire files. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation results for all successfully processed files with columns:\n",
    "            - File: Filename\n",
    "            - Correct: Number of correctly detected peaks\n",
    "            - Missed: Number of missed peaks\n",
    "            - Total_True: Total number of true peaks\n",
    "            - Total_Detected: Total number of detected peaks\n",
    "            - Accuracy_Percent: Detection accuracy percentage\n",
    "\n",
    "    Prints:\n",
    "        - Progress information and overall accuracy statistics\n",
    "        - Error messages for failed file processing\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(test_dir):\n",
    "        print(f\"Directory {test_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dat_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.dat')])\n",
    "    base_names = [f[:-4] for f in dat_files]\n",
    "\n",
    "    if not base_names:\n",
    "        print(f\"No .dat files found in {test_dir}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Evaluating R-peak detection on {len(base_names)} files from {test_dir}\")\n",
    "    if max_duration_hours:\n",
    "        print(f\"Processing {max_duration_hours:.1f} hours per file\")\n",
    "    else:\n",
    "        print(\"Processing complete files.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "    for name in tqdm(base_names, desc=\"Evaluating files\"):\n",
    "        file_result = evaluate_single_file_rpeak_detection(test_dir, name, fs=fs,\n",
    "                                                  max_duration_hours=max_duration_hours)\n",
    "        if 'Error' in file_result:\n",
    "            print(f\"Error processing {name}: {file_result['Error']}\")\n",
    "        else:\n",
    "            results.append(file_result)\n",
    "\n",
    "        # Force garbage collection\n",
    "        del file_result\n",
    "        gc.collect()\n",
    "\n",
    "    if results:\n",
    "        # Create DataFrame with proper rounding\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # Round numerical columns\n",
    "        numeric_columns = ['Accuracy_Percent']\n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].round(1)\n",
    "\n",
    "        print(\"\\nR-peak Detection Evaluation Results:\")\n",
    "        print(f\"Successfully processed {len(df)} out of {len(base_names)} files\")\n",
    "\n",
    "        # Calculate and display overall accuracy\n",
    "        total_correct = df['Correct'].sum()\n",
    "        total_true = df['Total_True'].sum()\n",
    "\n",
    "        if total_true > 0:\n",
    "            overall_accuracy = (total_correct / total_true) * 100\n",
    "            print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "            print(f\"Total True Peaks: {total_true}\")\n",
    "            print(f\"Total Correct Detections: {total_correct}\")\n",
    "        else:\n",
    "            print(\"No ground truth peaks found across all files\")\n",
    "\n",
    "        # Clean up before returning\n",
    "        del results\n",
    "        gc.collect()\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "649af853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. MAIN EXECUTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def run_ecg_analysis_pipeline(max_duration_hours: float = 1.0):\n",
    "    \"\"\"\n",
    "    Execute the complete ECG analysis pipeline with processing time limits.\n",
    "\n",
    "    Performs a comprehensive ECG analysis workflow including single file processing,\n",
    "    batch processing, R-peak detection evaluation, and anomaly detection on\n",
    "    specified data sources.\n",
    "\n",
    "    Args:\n",
    "        max_duration_hours (float): Maximum duration in hours to process per file.\n",
    "                                  Defaults to 1.0 hour to manage memory usage\n",
    "                                  and processing time.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing results from all pipeline steps:\n",
    "            - 'single_ecg': Results from single ECG file processing\n",
    "            - 'batch_ecg': Results from batch ECG file processing\n",
    "            - 'rpeak_evaluation': Results from R-peak detection evaluation\n",
    "            - 'anomaly_result': Results from anomaly detection evaluation\n",
    "\n",
    "    Note:\n",
    "        Pipeline processes data from 'source1' and 'source2' directories.\n",
    "        Processing time is limited per file to prevent memory overflow.\n",
    "    \"\"\"\n",
    "    print(\"ECG Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing up to {max_duration_hours:.1f} hours per file\")\n",
    "\n",
    "    # Step 1: Process single ECG file from source1\n",
    "    print(\"\\n1. Processing single ECG file from source1...\")\n",
    "    single_result = analyze_single_ecg_file(\"source1\", \"100001\",\n",
    "                                           plot=True, max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 2: Batch process files in source1\n",
    "    print(\"\\n2. Batch processing ECG files in source1...\")\n",
    "    batch_results = analyze_ecg_batch(\"source1\", plot_individual=False,\n",
    "                                    max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 3: Evaluate R-peak detection on source2\n",
    "    print(\"\\n3. Evaluating R-peak detection on source2...\")\n",
    "    rpeak_results = evaluate_rpeak_detection_batch(\"source2\", max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 4: Anomaly detection evaluation\n",
    "    print(\"\\n4. Sample anomaly detection on source1...\")\n",
    "    anomaly_result = None\n",
    "    if not batch_results.empty:\n",
    "        sample_record = batch_results['Record'].iloc[0]\n",
    "        anomaly_result = analyze_anomalies_in_file(\"source1\", sample_record,\n",
    "                                                       max_duration_hours=max_duration_hours)\n",
    "\n",
    "    print(\"\\nPipeline execution completed!\")\n",
    "\n",
    "    return {\n",
    "        'single_ecg': single_result,\n",
    "        'batch_ecg': batch_results,\n",
    "        'rpeak_evaluation': rpeak_results,\n",
    "        'anomaly_result': anomaly_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd8cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating R-peak detection on 18 files from source2\n",
      "Processing complete files.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating files:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.26 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11730944/11730944 [00:07<00:00, 1585244.81it/s]\n",
      "Evaluating files:   6%|▌         | 1/18 [00:10<03:06, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.20 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11520000/11520000 [00:04<00:00, 2585152.22it/s]\n",
      "Evaluating files:  11%|█         | 2/18 [00:18<02:18,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.15 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11354112/11354112 [00:04<00:00, 2430157.94it/s]\n",
      "Evaluating files:  17%|█▋        | 3/18 [00:27<02:17,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.07 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11051008/11051008 [00:10<00:00, 1063637.56it/s]\n",
      "Evaluating files:  22%|██▏       | 4/18 [00:45<02:58, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.32 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11960320/11960320 [00:07<00:00, 1589955.98it/s]\n",
      "Evaluating files:  28%|██▊       | 5/18 [00:57<02:37, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.15 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11327488/11327488 [00:07<00:00, 1572596.96it/s]\n",
      "Evaluating files:  33%|███▎      | 6/18 [01:11<02:34, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.07 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11046912/11046912 [00:04<00:00, 2420590.82it/s]\n",
      "Evaluating files:  39%|███▉      | 7/18 [01:21<02:10, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.13 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11284480/11284480 [00:04<00:00, 2440741.01it/s]\n",
      "Evaluating files:  44%|████▍     | 8/18 [01:30<01:51, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.02 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10866688/10866688 [00:04<00:00, 2334444.81it/s]\n",
      "Evaluating files:  50%|█████     | 9/18 [01:40<01:35, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2.96 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10659840/10659840 [00:09<00:00, 1162350.69it/s]\n",
      "Evaluating files:  56%|█████▌    | 10/18 [01:55<01:36, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.12 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11235328/11235328 [00:04<00:00, 2382629.07it/s]\n",
      "Evaluating files:  61%|██████    | 11/18 [02:03<01:15, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.32 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11960320/11960320 [00:05<00:00, 2091029.96it/s]\n",
      "Evaluating files:  67%|██████▋   | 12/18 [02:13<01:04, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.04 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10942464/10942464 [00:06<00:00, 1756501.88it/s]\n",
      "Evaluating files:  72%|███████▏  | 13/18 [02:23<00:51, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.05 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10967040/10967040 [00:03<00:00, 3167676.00it/s]\n",
      "Evaluating files:  78%|███████▊  | 14/18 [02:33<00:40, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.10 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11143168/11143168 [00:04<00:00, 2684855.56it/s]\n",
      "Evaluating files:  83%|████████▎ | 15/18 [02:39<00:27,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2.97 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10705920/10705920 [00:02<00:00, 4507226.40it/s]\n",
      "Evaluating files:  89%|████████▉ | 16/18 [02:44<00:15,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3.09 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 11139072/11139072 [00:03<00:00, 3597429.43it/s]\n",
      "Evaluating files:  94%|█████████▍| 17/18 [02:55<00:08,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2.97 hours in chunks of 10.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 10701824/10701824 [00:05<00:00, 2016541.94it/s]\n",
      "Evaluating files: 100%|██████████| 18/18 [03:05<00:00, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-peak Detection Evaluation Results:\n",
      "Successfully processed 18 out of 18 files\n",
      "Overall Accuracy: 26.93%\n",
      "Total True Peaks: 1806792\n",
      "Total Correct Detections: 486581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Missed</th>\n",
       "      <th>Total_True</th>\n",
       "      <th>Total_Detected</th>\n",
       "      <th>Accuracy_Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16265</td>\n",
       "      <td>29279</td>\n",
       "      <td>71676</td>\n",
       "      <td>100955</td>\n",
       "      <td>31152</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16272</td>\n",
       "      <td>27730</td>\n",
       "      <td>69416</td>\n",
       "      <td>97146</td>\n",
       "      <td>28687</td>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16273</td>\n",
       "      <td>26311</td>\n",
       "      <td>63786</td>\n",
       "      <td>90097</td>\n",
       "      <td>28217</td>\n",
       "      <td>29.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16420</td>\n",
       "      <td>27494</td>\n",
       "      <td>74942</td>\n",
       "      <td>102436</td>\n",
       "      <td>28400</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16483</td>\n",
       "      <td>26812</td>\n",
       "      <td>77749</td>\n",
       "      <td>104561</td>\n",
       "      <td>29159</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16539</td>\n",
       "      <td>29728</td>\n",
       "      <td>78946</td>\n",
       "      <td>108674</td>\n",
       "      <td>30331</td>\n",
       "      <td>27.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16773</td>\n",
       "      <td>26416</td>\n",
       "      <td>86481</td>\n",
       "      <td>112897</td>\n",
       "      <td>27496</td>\n",
       "      <td>23.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16786</td>\n",
       "      <td>30567</td>\n",
       "      <td>71172</td>\n",
       "      <td>101739</td>\n",
       "      <td>31186</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16795</td>\n",
       "      <td>26622</td>\n",
       "      <td>61056</td>\n",
       "      <td>87678</td>\n",
       "      <td>28159</td>\n",
       "      <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17052</td>\n",
       "      <td>26962</td>\n",
       "      <td>61040</td>\n",
       "      <td>88002</td>\n",
       "      <td>27647</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17453</td>\n",
       "      <td>26333</td>\n",
       "      <td>74840</td>\n",
       "      <td>101173</td>\n",
       "      <td>28061</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18177</td>\n",
       "      <td>28898</td>\n",
       "      <td>88106</td>\n",
       "      <td>117004</td>\n",
       "      <td>30601</td>\n",
       "      <td>24.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18184</td>\n",
       "      <td>27170</td>\n",
       "      <td>75502</td>\n",
       "      <td>102672</td>\n",
       "      <td>28730</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19088</td>\n",
       "      <td>24419</td>\n",
       "      <td>93461</td>\n",
       "      <td>117880</td>\n",
       "      <td>26224</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19090</td>\n",
       "      <td>24611</td>\n",
       "      <td>57342</td>\n",
       "      <td>81953</td>\n",
       "      <td>27620</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19093</td>\n",
       "      <td>25862</td>\n",
       "      <td>57808</td>\n",
       "      <td>83670</td>\n",
       "      <td>27321</td>\n",
       "      <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19140</td>\n",
       "      <td>26208</td>\n",
       "      <td>70784</td>\n",
       "      <td>96992</td>\n",
       "      <td>28094</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19830</td>\n",
       "      <td>25159</td>\n",
       "      <td>86104</td>\n",
       "      <td>111263</td>\n",
       "      <td>26259</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File  Correct  Missed  Total_True  Total_Detected  Accuracy_Percent\n",
       "0   16265    29279   71676      100955           31152              29.0\n",
       "1   16272    27730   69416       97146           28687              28.5\n",
       "2   16273    26311   63786       90097           28217              29.2\n",
       "3   16420    27494   74942      102436           28400              26.8\n",
       "4   16483    26812   77749      104561           29159              25.6\n",
       "5   16539    29728   78946      108674           30331              27.4\n",
       "6   16773    26416   86481      112897           27496              23.4\n",
       "7   16786    30567   71172      101739           31186              30.0\n",
       "8   16795    26622   61056       87678           28159              30.4\n",
       "9   17052    26962   61040       88002           27647              30.6\n",
       "10  17453    26333   74840      101173           28061              26.0\n",
       "11  18177    28898   88106      117004           30601              24.7\n",
       "12  18184    27170   75502      102672           28730              26.5\n",
       "13  19088    24419   93461      117880           26224              20.7\n",
       "14  19090    24611   57342       81953           27620              30.0\n",
       "15  19093    25862   57808       83670           27321              30.9\n",
       "16  19140    26208   70784       96992           28094              27.0\n",
       "17  19830    25159   86104      111263           26259              22.6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "# Run the memory-efficient pipeline\n",
    "# results = run_ecg_analysis_pipeline(max_duration_hours=0.5)\n",
    "\n",
    "# Or run individual components with limited duration:\n",
    "# analyze_single_ecg_file(\"source1\", \"100001\", max_duration_hours=None)\n",
    "# analyze_ecg_batch(\"source1\", max_duration_hours=None)\n",
    "evaluate_rpeak_detection_batch(\"source2\", max_duration_hours=None)\n",
    "# analyze_anomalies_in_file(\"source1\", \"100001\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
