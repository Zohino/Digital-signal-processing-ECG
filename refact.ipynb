{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d545dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG Analysis Pipeline: Memory-Efficient R-peak Detection and Anomaly Detection\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Download datasets if needed (commented out to avoid re-downloading)\n",
    "# wfdb.dl_database(\"butqdb\", \"source1\", keep_subdirs=False)\n",
    "# wfdb.dl_database(\"nsrdb\", \"source2\", keep_subdirs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96467c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. MEMORY-EFFICIENT SIGNAL PREPROCESSING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def center_signal(signal: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Subtract mean to zero-center the signal.\"\"\"\n",
    "    return signal - np.mean(signal)\n",
    "\n",
    "def highpass_filter(signal: np.ndarray, fs: int, cutoff: float = 0.5, order: int = 1) -> np.ndarray:\n",
    "    \"\"\"Apply Butterworth high-pass filter to remove baseline drift.\"\"\"\n",
    "    b, a = butter(order, cutoff / (0.5 * fs), btype='high')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def preprocess_ecg(signal: np.ndarray, fs: int) -> np.ndarray:\n",
    "    \"\"\"Complete ECG preprocessing pipeline.\"\"\"\n",
    "    signal = center_signal(signal)\n",
    "    signal = highpass_filter(signal, fs, cutoff=0.5, order=2)\n",
    "    return signal\n",
    "\n",
    "class ECGStreamProcessor:\n",
    "    \"\"\"Memory-efficient ECG stream processor using chunked reading.\"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000, chunk_duration_sec: int = 300):\n",
    "        \"\"\"\n",
    "        Initialize processor.\n",
    "\n",
    "        Args:\n",
    "            fs: Sampling frequency\n",
    "            chunk_duration_sec: Duration of each chunk in seconds (default 5 minutes)\n",
    "        \"\"\"\n",
    "        self.fs = fs\n",
    "        self.chunk_size = chunk_duration_sec * fs\n",
    "        self.overlap_size = int(fs * 1.0)  # 1 second overlap for continuity\n",
    "\n",
    "    def process_file_chunked(self, record_path: str, processor_func,\n",
    "                           max_duration_hours: float = None) -> List:\n",
    "        \"\"\"\n",
    "        Process ECG file in chunks to avoid memory overflow.\n",
    "\n",
    "        Args:\n",
    "            record_path: Path to ECG record\n",
    "            processor_func: Function to apply to each chunk\n",
    "            max_duration_hours: Maximum duration to process (None for full file)\n",
    "\n",
    "        Returns:\n",
    "            List of results from each chunk\n",
    "        \"\"\"\n",
    "        # Get file info\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * self.fs)\n",
    "            total_samples = min(total_samples, max_samples)\n",
    "\n",
    "        results = []\n",
    "        processed_samples = 0\n",
    "\n",
    "        print(f\"Processing {total_samples/self.fs/3600:.2f} hours in chunks of {self.chunk_size/self.fs/60:.1f} minutes\")\n",
    "\n",
    "        # Process in chunks\n",
    "        with tqdm(total=total_samples, desc=\"Processing chunks\") as pbar:\n",
    "            while processed_samples < total_samples:\n",
    "                # Calculate chunk boundaries\n",
    "                start_sample = max(0, processed_samples - self.overlap_size)\n",
    "                end_sample = min(total_samples, processed_samples + self.chunk_size)\n",
    "\n",
    "                # Read chunk\n",
    "                record = wfdb.rdrecord(record_path, sampfrom=start_sample, sampto=end_sample)\n",
    "                chunk = record.p_signal[:, 0]\n",
    "\n",
    "                # Preprocess chunk\n",
    "                chunk_processed = preprocess_ecg(chunk, self.fs)\n",
    "\n",
    "                # Apply processor function\n",
    "                chunk_result = processor_func(chunk_processed, start_sample)\n",
    "                results.append(chunk_result)\n",
    "\n",
    "                # Update progress\n",
    "                processed_samples += self.chunk_size\n",
    "                pbar.update(min(self.chunk_size, end_sample - (processed_samples - self.chunk_size)))\n",
    "\n",
    "                # Force garbage collection\n",
    "                del chunk, chunk_processed, record\n",
    "                gc.collect()\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aedbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. MEMORY-EFFICIENT R-PEAK DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_rpeaks_simple(signal: np.ndarray, fs: int, height_percentile: float = 98,\n",
    "                        min_distance_ms: float = 200) -> np.ndarray:\n",
    "    \"\"\"Detect R-peaks using simple peak detection with dynamic threshold.\"\"\"\n",
    "    height = np.percentile(signal, height_percentile)\n",
    "    min_distance = int(min_distance_ms * fs / 1000)\n",
    "    peaks, _ = find_peaks(signal, height=height, distance=min_distance)\n",
    "    return peaks\n",
    "\n",
    "class RPeakDetector:\n",
    "    \"\"\"Memory-efficient R-peak detector for long ECG signals.\"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000):\n",
    "        self.fs = fs\n",
    "        self.all_peaks = []\n",
    "\n",
    "    def process_chunk(self, chunk: np.ndarray, chunk_start: int) -> Dict:\n",
    "        \"\"\"Process a single chunk for R-peak detection.\"\"\"\n",
    "        # Detect peaks in chunk\n",
    "        peaks = detect_rpeaks_simple(chunk, self.fs)\n",
    "\n",
    "        # Adjust peak indices to global position\n",
    "        global_peaks = peaks + chunk_start\n",
    "\n",
    "        # Store peaks (avoiding overlap region)\n",
    "        overlap_samples = int(self.fs * 1.0)  # 1 second\n",
    "        if chunk_start > 0:\n",
    "            # Skip overlap region for non-first chunks\n",
    "            valid_mask = peaks >= overlap_samples\n",
    "            global_peaks = global_peaks[valid_mask]\n",
    "\n",
    "        self.all_peaks.extend(global_peaks)\n",
    "\n",
    "        return {\n",
    "            'chunk_start': chunk_start,\n",
    "            'chunk_peaks': len(peaks),\n",
    "            'valid_peaks': len(global_peaks)\n",
    "        }\n",
    "\n",
    "    def get_results(self) -> np.ndarray:\n",
    "        \"\"\"Get all detected R-peaks.\"\"\"\n",
    "        return np.array(sorted(self.all_peaks))\n",
    "\n",
    "def compute_bpm_windowed_efficient(rpeaks: np.ndarray, fs: int,\n",
    "                                 total_duration_sec: float, window_sec: int = 15) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Memory-efficient BPM computation.\"\"\"\n",
    "    if len(rpeaks) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    n_windows = int(total_duration_sec // window_sec)\n",
    "    bpm_values = np.zeros(n_windows)\n",
    "\n",
    "    # Use searchsorted for efficient counting\n",
    "    window_starts = np.arange(n_windows) * window_sec * fs\n",
    "    window_ends = window_starts + window_sec * fs\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        start_idx = np.searchsorted(rpeaks, window_starts[i], side='left')\n",
    "        end_idx = np.searchsorted(rpeaks, window_ends[i], side='right')\n",
    "        peaks_in_window = end_idx - start_idx\n",
    "        bpm_values[i] = peaks_in_window * 60 / window_sec\n",
    "\n",
    "    time_minutes = np.arange(n_windows) * (window_sec / 60)\n",
    "    return bpm_values, time_minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. MEMORY-EFFICIENT SINGLE FILE ECG PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_single_ecg_file_efficient(data_dir: str, record_name: str, plot: bool = True,\n",
    "                                    max_duration_hours: float = 1.0) -> Dict:\n",
    "    \"\"\"Memory-efficient processing of a single ECG file.\"\"\"\n",
    "\n",
    "    record_path = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "    results = {\n",
    "        'record_name': record_name,\n",
    "        'success': False,\n",
    "        'fs': None,\n",
    "        'total_samples': None,\n",
    "        'detected_rpeaks': None,\n",
    "        'average_bpm': None,\n",
    "        'bpm_values': None,\n",
    "        'time_minutes': None,\n",
    "        'processing_duration_hours': None,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get file info\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        fs = header.fs\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        processing_duration_hours = processing_samples / fs / 3600\n",
    "\n",
    "        print(f\"  Total file duration: {total_samples/fs/3600:.2f} hours\")\n",
    "        print(f\"  Processing duration: {processing_duration_hours:.2f} hours\")\n",
    "\n",
    "        # Initialize stream processor and R-peak detector\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=300)  # 5-minute chunks\n",
    "        detector = RPeakDetector(fs=fs)\n",
    "\n",
    "        # Process file in chunks\n",
    "        chunk_results = processor.process_file_chunked(\n",
    "            record_path, detector.process_chunk, max_duration_hours\n",
    "        )\n",
    "\n",
    "        # Get all detected R-peaks\n",
    "        rpeaks = detector.get_results()\n",
    "\n",
    "        # Compute BPM efficiently\n",
    "        total_duration_sec = processing_samples / fs\n",
    "        bpm_values, time_minutes = compute_bpm_windowed_efficient(\n",
    "            rpeaks, fs, total_duration_sec\n",
    "        )\n",
    "        avg_bpm = np.mean(bpm_values) if len(bpm_values) > 0 else 0\n",
    "\n",
    "        # Update results\n",
    "        results.update({\n",
    "            'fs': fs,\n",
    "            'total_samples': processing_samples,\n",
    "            'detected_rpeaks': len(rpeaks),\n",
    "            'average_bpm': avg_bpm,\n",
    "            'bpm_values': bpm_values,\n",
    "            'time_minutes': time_minutes,\n",
    "            'processing_duration_hours': processing_duration_hours,\n",
    "            'success': True\n",
    "        })\n",
    "\n",
    "        # Plot results\n",
    "        if plot and len(bpm_values) > 0:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(time_minutes, bpm_values, \"-\", linewidth=1, alpha=0.8)\n",
    "            plt.title(f\"BPM Over Time - {record_name} ({processing_duration_hours:.1f}h)\")\n",
    "            plt.xlabel(\"Time (minutes)\")\n",
    "            plt.ylabel(\"BPM\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"Record: {record_name}\")\n",
    "        print(f\"Sampling rate: {fs} Hz\")\n",
    "        print(f\"Processed samples: {processing_samples:,}\")\n",
    "        print(f\"Detected R-peaks: {len(rpeaks):,}\")\n",
    "        print(f\"Average BPM: {avg_bpm:.1f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {record_name}: {str(e)}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. MEMORY-EFFICIENT BATCH ECG PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_ecg_batch_efficient(data_dir: str, plot_individual: bool = False,\n",
    "                              max_duration_hours: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"Memory-efficient batch processing of ECG files.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_files = os.listdir(data_dir)\n",
    "    ecg_files = [f for f in all_files if f.endswith('_ECG.hea')]\n",
    "    record_names = sorted([f.replace('_ECG.hea', '') for f in ecg_files])\n",
    "\n",
    "    print(f\"Processing {len(record_names)} ECG records from {data_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(record_names)}: {record_name}\")\n",
    "\n",
    "        result = process_single_ecg_file_efficient(\n",
    "            data_dir, record_name, plot=plot_individual,\n",
    "            max_duration_hours=max_duration_hours\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "        # Force garbage collection between files\n",
    "        gc.collect()\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    successful_results = [r for r in results if r.get('success', False)]\n",
    "\n",
    "    if successful_results:\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                'Record': r['record_name'],\n",
    "                'Sampling_Rate_Hz': r['fs'],\n",
    "                'Total_Samples': r['total_samples'],\n",
    "                'Detected_R_peaks': r['detected_rpeaks'],\n",
    "                'Average_BPM': r['average_bpm'],\n",
    "                'Duration_Hours': r['processing_duration_hours']\n",
    "            }\n",
    "            for r in successful_results\n",
    "        ])\n",
    "\n",
    "        print(\"\\nBatch Processing Summary:\")\n",
    "        print(df.round(1))\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d046f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 5. MEMORY-EFFICIENT ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Memory-efficient anomaly detector for long ECG signals.\"\"\"\n",
    "\n",
    "    def __init__(self, fs: int = 1000):\n",
    "        self.fs = fs\n",
    "        self.all_anomalies = []\n",
    "\n",
    "    def detect_anomalies_amplitude(self, signal: np.ndarray, k: float = 3.7) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Detect anomalies based on amplitude threshold.\"\"\"\n",
    "        mean_val = signal.mean()\n",
    "        std_val = signal.std()\n",
    "        threshold = mean_val + k * std_val\n",
    "        anomaly_indices = np.where(np.abs(signal) > threshold)[0]\n",
    "        return anomaly_indices, threshold\n",
    "\n",
    "    def detect_anomalies_rolling(self, signal: np.ndarray, window: int = 1000, threshold: float = 15.5) -> np.ndarray:\n",
    "        \"\"\"Detect anomalies using rolling statistics.\"\"\"\n",
    "        if len(signal) < window:\n",
    "            window = len(signal) // 2\n",
    "\n",
    "        rolling_mean = np.convolve(signal, np.ones(window) / window, mode='same')\n",
    "        rolling_std = np.sqrt(np.convolve((signal - rolling_mean)**2, np.ones(window) / window, mode='same'))\n",
    "\n",
    "        # Avoid division by zero\n",
    "        rolling_std = np.maximum(rolling_std, 1e-8)\n",
    "        anomaly_indices = np.where(np.abs(signal - rolling_mean) > threshold * rolling_std)[0]\n",
    "        return anomaly_indices\n",
    "\n",
    "    def detect_anomalies_iqr(self, signal: np.ndarray, k: float = 15.0) -> np.ndarray:\n",
    "        \"\"\"Detect anomalies using IQR-based method.\"\"\"\n",
    "        q1, q3 = np.percentile(signal, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:  # Handle constant signal\n",
    "            return np.array([])\n",
    "        lower_bound = q1 - k * iqr\n",
    "        upper_bound = q3 + k * iqr\n",
    "        anomaly_indices = np.where((signal < lower_bound) | (signal > upper_bound))[0]\n",
    "        return anomaly_indices\n",
    "\n",
    "    def process_chunk(self, chunk: np.ndarray, chunk_start: int) -> Dict:\n",
    "        \"\"\"Process a single chunk for anomaly detection.\"\"\"\n",
    "        # Apply multiple detection methods\n",
    "        methods_results = []\n",
    "\n",
    "        # Amplitude-based detection\n",
    "        amp_anomalies, threshold = self.detect_anomalies_amplitude(chunk)\n",
    "        methods_results.append(amp_anomalies)\n",
    "\n",
    "        # Rolling statistics detection\n",
    "        rolling_anomalies = self.detect_anomalies_rolling(chunk)\n",
    "        methods_results.append(rolling_anomalies)\n",
    "\n",
    "        # IQR-based detection\n",
    "        iqr_anomalies = self.detect_anomalies_iqr(chunk)\n",
    "        methods_results.append(iqr_anomalies)\n",
    "\n",
    "        # Combine results\n",
    "        if any(len(arr) > 0 for arr in methods_results):\n",
    "            combined_anomalies = np.unique(np.concatenate([arr for arr in methods_results if len(arr) > 0]))\n",
    "        else:\n",
    "            combined_anomalies = np.array([])\n",
    "\n",
    "        # Adjust indices to global position\n",
    "        global_anomalies = combined_anomalies + chunk_start\n",
    "\n",
    "        # Store anomalies (avoiding overlap region)\n",
    "        overlap_samples = int(self.fs * 1.0)  # 1 second\n",
    "        if chunk_start > 0:\n",
    "            # Skip overlap region for non-first chunks\n",
    "            valid_mask = combined_anomalies >= overlap_samples\n",
    "            global_anomalies = global_anomalies[valid_mask]\n",
    "\n",
    "        self.all_anomalies.extend(global_anomalies)\n",
    "\n",
    "        return {\n",
    "            'chunk_start': chunk_start,\n",
    "            'chunk_anomalies': len(combined_anomalies),\n",
    "            'valid_anomalies': len(global_anomalies),\n",
    "            'amplitude_threshold': threshold\n",
    "        }\n",
    "\n",
    "    def get_results(self) -> np.ndarray:\n",
    "        \"\"\"Get all detected anomalies.\"\"\"\n",
    "        return np.array(sorted(self.all_anomalies))\n",
    "\n",
    "def evaluate_anomaly_detection_file_efficient(data_dir: str, record_name: str,\n",
    "                                             max_duration_hours: float = 1.0) -> Dict:\n",
    "    \"\"\"Memory-efficient anomaly detection evaluation for a single file.\"\"\"\n",
    "\n",
    "    record_path = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "    results = {\n",
    "        'record_name': record_name,\n",
    "        'success': False,\n",
    "        'detected_anomalies': 0,\n",
    "        'anomaly_rate_per_hour': 0,\n",
    "        'processing_duration_hours': 0,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get file info\n",
    "        header = wfdb.rdheader(record_path)\n",
    "        fs = header.fs\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        processing_duration_hours = processing_samples / fs / 3600\n",
    "\n",
    "        # Initialize stream processor and anomaly detector\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=300)  # 5-minute chunks\n",
    "        detector = AnomalyDetector(fs=fs)\n",
    "\n",
    "        # Process file in chunks\n",
    "        chunk_results = processor.process_file_chunked(\n",
    "            record_path, detector.process_chunk, max_duration_hours\n",
    "        )\n",
    "\n",
    "        # Get all detected anomalies\n",
    "        anomalies = detector.get_results()\n",
    "        anomaly_rate = len(anomalies) / processing_duration_hours if processing_duration_hours > 0 else 0\n",
    "\n",
    "        results.update({\n",
    "            'success': True,\n",
    "            'detected_anomalies': len(anomalies),\n",
    "            'anomaly_rate_per_hour': anomaly_rate,\n",
    "            'processing_duration_hours': processing_duration_hours\n",
    "        })\n",
    "\n",
    "        print(f\"Record: {record_name}\")\n",
    "        print(f\"Detected anomalies: {len(anomalies):,}\")\n",
    "        print(f\"Anomaly rate: {anomaly_rate:.1f} per hour\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"Error processing {record_name}: {str(e)}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211aae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. MEMORY-EFFICIENT R-PEAK EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rpeak_detection(true_peaks: np.ndarray, detected_peaks: np.ndarray,\n",
    "                           tolerance: float = 1e-4) -> Tuple[int, int, int]:\n",
    "    \"\"\"Vectorized evaluation of R-peak detection accuracy.\"\"\"\n",
    "    if len(true_peaks) == 0 or len(detected_peaks) == 0:\n",
    "        return 0, len(true_peaks), len(detected_peaks)\n",
    "\n",
    "    true_peaks = np.array(true_peaks)\n",
    "    detected_peaks = np.array(detected_peaks)\n",
    "\n",
    "    rel_diff = np.abs(true_peaks[:, None] / detected_peaks[None, :] - 1)\n",
    "    matched = np.any(rel_diff < tolerance, axis=1)\n",
    "    correct = int(np.sum(matched))\n",
    "    return correct, len(true_peaks), len(detected_peaks)\n",
    "\n",
    "def process_file_safe_efficient(test_dir: str, base_name: str, fs: int = 1000,\n",
    "                               max_duration_hours: float = 1.0) -> Dict:\n",
    "    \"\"\"Memory-efficient evaluation of a single file.\"\"\"\n",
    "\n",
    "    file_path = os.path.join(test_dir, base_name)\n",
    "    out = {'File': base_name}\n",
    "\n",
    "    try:\n",
    "        # Get file info first\n",
    "        header = wfdb.rdheader(file_path)\n",
    "        total_samples = header.sig_len\n",
    "\n",
    "        if max_duration_hours:\n",
    "            max_samples = int(max_duration_hours * 3600 * fs)\n",
    "            processing_samples = min(total_samples, max_samples)\n",
    "        else:\n",
    "            processing_samples = total_samples\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = wfdb.rdann(file_path, 'atr')\n",
    "        true_peaks = annotations.sample\n",
    "        true_peaks = true_peaks[true_peaks < processing_samples]\n",
    "\n",
    "        # Process ECG in chunks\n",
    "        processor = ECGStreamProcessor(fs=fs, chunk_duration_sec=300)\n",
    "        detector = RPeakDetector(fs=fs)\n",
    "\n",
    "        chunk_results = processor.process_file_chunked(\n",
    "            file_path, detector.process_chunk, max_duration_hours\n",
    "        )\n",
    "\n",
    "        detected_peaks = detector.get_results()\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        correct, total_true, total_detected = evaluate_rpeak_detection(true_peaks, detected_peaks)\n",
    "        accuracy = (correct / total_true * 100) if total_true > 0 else 0\n",
    "\n",
    "        out.update({\n",
    "            'Correct': correct,\n",
    "            'Missed': total_true - correct,\n",
    "            'Total_True': total_true,\n",
    "            'Total_Detected': total_detected,\n",
    "            'Accuracy_Percent': accuracy\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        out['Error'] = str(e)\n",
    "\n",
    "    return out\n",
    "\n",
    "def evaluate_rpeak_batch_efficient(test_dir: str, fs: int = 1000,\n",
    "                                 max_duration_hours: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"Memory-efficient R-peak detection evaluation.\"\"\"\n",
    "\n",
    "    if not os.path.isdir(test_dir):\n",
    "        print(f\"Directory {test_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dat_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.dat')])\n",
    "    base_names = [f[:-4] for f in dat_files]\n",
    "\n",
    "    print(f\"Evaluating R-peak detection on {len(base_names)} files from {test_dir}\")\n",
    "    print(f\"Processing {max_duration_hours:.1f} hours per file\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "    for name in tqdm(base_names, desc=\"Evaluating\"):\n",
    "        res = process_file_safe_efficient(test_dir, name, fs=fs,\n",
    "                                        max_duration_hours=max_duration_hours)\n",
    "        if 'Error' in res:\n",
    "            print(f\"Error processing {name}: {res['Error']}\")\n",
    "        else:\n",
    "            results.append(res)\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results).round(1)\n",
    "        print(\"R-peak Detection Evaluation Results:\")\n",
    "        print(df)\n",
    "\n",
    "        overall = df['Correct'].sum() / df['Total_True'].sum() * 100 if df['Total_True'].sum() > 0 else 0\n",
    "        print(f\"\\nOverall Accuracy: {overall:.2f}%\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No files were successfully processed.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649af853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN EXECUTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def run_ecg_analysis_pipeline_efficient(max_duration_hours: float = 1.0):\n",
    "    \"\"\"\n",
    "    Run the complete memory-efficient ECG analysis pipeline.\n",
    "\n",
    "    Args:\n",
    "        max_duration_hours: Maximum duration to process per file (default 1 hour)\n",
    "    \"\"\"\n",
    "    print(\"Memory-Efficient ECG Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing up to {max_duration_hours:.1f} hours per file\")\n",
    "\n",
    "    # Step 1: Process single ECG file from source1\n",
    "    print(\"\\n1. Processing single ECG file from source1...\")\n",
    "    single_result = process_single_ecg_file_efficient(\"source1\", \"100001\",\n",
    "                                                    plot=True, max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 2: Batch process files in source1\n",
    "    print(\"\\n2. Batch processing ECG files in source1...\")\n",
    "    batch_results = process_ecg_batch_efficient(\"source1\", plot_individual=False,\n",
    "                                              max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 3: Evaluate R-peak detection on source2\n",
    "    print(\"\\n3. Evaluating R-peak detection on source2...\")\n",
    "    rpeak_results = evaluate_rpeak_batch_efficient(\"source2\", max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # Step 4: Anomaly detection evaluation\n",
    "    print(\"\\n4. Sample anomaly detection on source1...\")\n",
    "    if not batch_results.empty:\n",
    "        sample_record = batch_results['Record'].iloc[0]\n",
    "        anomaly_result = evaluate_anomaly_detection_file_efficient(\"source1\", sample_record,\n",
    "                                                                  max_duration_hours=max_duration_hours)\n",
    "\n",
    "    print(\"\\nMemory-efficient pipeline completed!\")\n",
    "\n",
    "    return {\n",
    "        'single_ecg': single_result,\n",
    "        'batch_ecg': batch_results,\n",
    "        'rpeak_evaluation': rpeak_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "# Run the memory-efficient pipeline\n",
    "# results = run_ecg_analysis_pipeline_efficient(max_duration_hours=1.0)\n",
    "\n",
    "# Or run individual components with limited duration:\n",
    "# process_single_ecg_file_efficient(\"source1\", \"100001\", max_duration_hours=None)\n",
    "# process_ecg_batch_efficient(\"source1\", max_duration_hours=None)\n",
    "evaluate_rpeak_batch_efficient(\"source2\", max_duration_hours=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the memory-efficient ECG pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# Test with limited duration to avoid memory issues\n",
    "print(\"Testing memory-efficient ECG pipeline...\")\n",
    "\n",
    "# Process just 30 minutes of data for testing\n",
    "test_duration_hours = 1  # 30 minutes\n",
    "\n",
    "# Test single file processing\n",
    "try:\n",
    "    print(\"\\nTesting single file processing...\")\n",
    "    result = process_single_ecg_file_efficient(\"source1\", \"100001\",\n",
    "                                              plot=True,\n",
    "                                              max_duration_hours=test_duration_hours)\n",
    "    print(f\"Single file test: {'SUCCESS' if result['success'] else 'FAILED'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Single file test failed: {e}\")\n",
    "\n",
    "# Test batch processing (limit to first 2 files)\n",
    "try:\n",
    "    print(f\"\\nTesting batch processing with {test_duration_hours} hours per file...\")\n",
    "\n",
    "    # Get list of files\n",
    "    if os.path.exists(\"source1\"):\n",
    "        files = [f for f in os.listdir(\"source1\") if f.endswith('_ECG.hea')]\n",
    "        print(f\"Found {len(files)} ECG files\")\n",
    "\n",
    "        # Process first file only for testing\n",
    "        if files:\n",
    "            first_file = files[0].replace('_ECG.hea', '')\n",
    "            print(f\"Processing first file: {first_file}\")\n",
    "\n",
    "            result = process_single_ecg_file_efficient(\"source1\", first_file,\n",
    "                                                      plot=False,\n",
    "                                                      max_duration_hours=test_duration_hours)\n",
    "            print(f\"Test completed: {'SUCCESS' if result['success'] else 'FAILED'}\")\n",
    "        else:\n",
    "            print(\"No ECG files found in source1\")\n",
    "    else:\n",
    "        print(\"source1 directory not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Batch processing test failed: {e}\")\n",
    "\n",
    "# Test R-peak evaluation if source2 exists\n",
    "try:\n",
    "    if os.path.exists(\"source2\"):\n",
    "        print(f\"\\nTesting R-peak evaluation with {test_duration_hours} hours per file...\")\n",
    "        results = evaluate_rpeak_batch_efficient(\"source2\", max_duration_hours=test_duration_hours)\n",
    "        print(f\"R-peak evaluation: {'SUCCESS' if not results.empty else 'NO DATA'}\")\n",
    "    else:\n",
    "        print(\"source2 directory not found - skipping R-peak evaluation\")\n",
    "except Exception as e:\n",
    "    print(f\"R-peak evaluation test failed: {e}\")\n",
    "\n",
    "print(\"\\nMemory usage test completed!\")\n",
    "print(f\"Processed up to {test_duration_hours} hours per file to conserve memory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
