{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dcd3e2",
   "metadata": {},
   "source": [
    "# Seminární práce I z předmětu Počítačové zpracování signálu (KI/PZS)\n",
    "\n",
    "**Autor**: Martin Žoha\n",
    "\n",
    "**Datum**: 21. 6. 2025\n",
    "\n",
    "## Úvod\n",
    "\n",
    "Seminární práce se zabývá zpracováním EKG signálů. Zdrojem signálů jsou databáze\n",
    " [Physionet Brno University of Technology ECG Quality Database (BUT QDB)](https://physionet.org/content/butqdb/1.0.0/) a \n",
    " [MIT-BIH Normal Sinus Rhythm Database (NSRDB)](https://physionet.org/content/nsrdb/1.0.0/). Samotné zpracování je rozděleno do dvou hlavních částí. Předmětem první části je načtení dat do prostředí Python a výpočet tepové frekvence z načtených dat. Druhá část práce je zaměřena na detekci anomálií v signálech. Kód k jednotlivým částem je rozdělen do logických celků a připraven i k použití bez doprovodného komentáře. Všechny funkce a třídy jsou opatřeny extenzivní dokumentací ve stylu Google. Vybrané části jsou doplněny komentáři. Celý notebook je připraven k jednorázovému spuštění, ale případné úpravy jsou možné.\n",
    "\n",
    "## Výpočet tepové frekvence z EKG signálu\n",
    "\n",
    "### Zadání první části\n",
    "\n",
    "Ve zdrojové databázi najdete celkem 18 měření EKG signálu pro různé věkové skupiny. Signál\n",
    "obsahuje různé anomálie a nemusí být vždy centralizován podle vodorovné osy. EKG signál\n",
    "obsahuje dominantní peaky, které se nazývají R vrcholy. Vzdálenost těchto vrcholů určuje dobu\n",
    "mezi jednotlivými tepy. Počet tepů za minutu je tedy počet R vrcholů v signálu o délce jedné\n",
    "minuty. Navrhněte algoritmus, který bude automaticky detekovat počet R vrcholů v EKG\n",
    "signálech a prezentujte tepovou frekvenci při jednotlivých jízdách/měřeních. Vás algoritmus\n",
    "následně otestujte na databázi MIT-BIH https://physionet.org/content/nsrdb/1.0.0/ a\n",
    "prezentujte jeho úspěšnost vzhledem k anotovaným datům z databáze.\n",
    "\n",
    "### Obecný postup zpracování první části\n",
    "\n",
    "Data jsou načtena pomocí knihovny [Waveform Database Software Package (WFDB) for Python](https://physionet.org/content/wfdb-python/4.1.0/) a předzpracována pro následné použití. V signálu jsou detekovány pozice R vrcholů. Ze zjištěných pozic je určen rozestup R vrcholů a vypočtena tepová frekvence v tepech za minutu. Přesnost výsledků je ověřena na anotovaných datech MIT-BIH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d545dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG Analysis Pipeline: R-peak Detection and Anomaly Detection\n",
    "# =============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import butter, find_peaks, sosfiltfilt\n",
    "from typing import Dict, List, Tuple\n",
    "import wfdb\n",
    "\n",
    "\n",
    "# Download datasets\n",
    "# wfdb.dl_database(\"butqdb\", \"source1\", keep_subdirs=False)\n",
    "# wfdb.dl_database(\"nsrdb\", \"source2\", keep_subdirs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe945b79",
   "metadata": {},
   "source": [
    "### 1. Předzpracování signálu\n",
    "\n",
    "V této fázi jsou připraveny funkce k předzpracování signálu. Funkce `filter_ecg` slouží k vyčištění EKG signálu od rušení a šumu, které ztěžují jeho správnou analýzu. Používá k tomu Butterworthův filtr, což je druh filtru, který propustí jen určité frekvence tím, že zeslabí nebo úplně potlačí složky signálu, které leží mimo zadané frekvenční pásmo. V tomto případě je zvoleno frekvenční pásmo mezi 0.5 a 40 Hz. Řád filtru určuje, jak ostrý je přechod mezi povolenými a filtrovanými frekvencemi. Čím vyšší řád je, tím ostřejší přechod. Zvolený řád je kompromisem mezi čitelností signálu a jeho poškozením. Tyto hodnoty jsou zvoleny iterativně. Postupně byly zkoušeny hodnoty, aby v následném hodnocení detekce R vrcholů vycházely co nejlepší výsledky.\n",
    "\n",
    "Tento filtr má hladký průběh a nezpůsobuje ostré změny ve tvaru signálu. Filtr je vytvořen ve stabilní podobě, aby dobře fungoval i pro delší nebo složitější signály. Aby se nezměnil tvar EKG vln (například QRS komplex), používá se funkce `sosfiltfilt`, která filtr aplikuje dvakrát – nejdřív dopředu a pak zpátky. Díky tomu nedochází k posunu ve tvaru vln. Výsledkem je EKG signál, který je zbavený pomalých výkyvů (například způsobených pohybem nebo dýcháním) i vysokofrekvenčního šumu (např. elektrické rušení), a je tak připravený pro další analýzu.\n",
    "\n",
    "Funkce `normalize_signal` přijme filtrovaný signál a zarovná ho tak, aby měl průměrnou hodnotu nula a standardní odchylku jedna. Dělá to tak, že od každé hodnoty v signálu odečte celkový průměr a pak výsledek vydělí směrodatnou odchylkou - tento postup se nazývá z-score normalizace. Hlavní důvod, proč se to dělá, je odstranění rozdílů v amplitudě mezi různými záznamy, zatímco křivka srdečního rytmu zůstane stejná, což je klíčové pro správnou analýzu.\n",
    "\n",
    "V těchto funkcích jsou využity předpřipravené funkce `butter` a `sosfiltfilt` z knihovny scipy. Tato implementace je zvolena kvůli čitelnosti kódu, rychlosti zpracování a přesnosti výsledků.\n",
    "\n",
    "Výsledkem výše uvedeného procesu je vertikálně zarovnaný a normalizovaný signál zbavený šumu, což je žádoucí pro detekci R vrcholů. Toto komplexní předzpracování naopak negativně ovlivňuje detekci anomálií v druhé části práce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96467c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  1. SIGNAL PREPROCESSING\n",
    "# =============================================================================\n",
    "def filter_ecg(signal: np.ndarray, fs: int, low: float = 0.5,\n",
    "               high: float = 40.0) -> np.ndarray:\n",
    "    \"\"\"Apply bandpass filter to ECG signal to remove noise and artifacts.\n",
    "\n",
    "    Applies a 4th-order Butterworth bandpass filter to remove baseline drift\n",
    "    (low-frequency noise) and high-frequency noise from ECG signals. The\n",
    "    default frequency range (0.5-40 Hz) captures the diagnostically relevant\n",
    "    components of ECG signals while filtering out motion artifacts and\n",
    "    power line interference.\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): Input ECG signal as 1D numpy array.\n",
    "        fs (int): Sampling frequency of the signal in Hz.\n",
    "        low (float, optional): Low cutoff frequency in Hz. Defaults to 0.5.\n",
    "            Removes baseline drift and respiratory artifacts.\n",
    "        high (float, optional): High cutoff frequency in Hz. Defaults to 40.0.\n",
    "            Removes high-frequency noise while preserving QRS morphology.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Filtered ECG signal with same shape as input.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If low >= high or if frequencies are outside valid range.\n",
    "        TypeError: If signal is not a numpy array or fs is not numeric.\n",
    "\n",
    "    Example:\n",
    "        >>> ecg_data = np.random.randn(1000)  # Sample ECG signal\n",
    "        >>> filtered_ecg = filter_ecg(ecg_data, fs=250)\n",
    "        >>> print(f\"Original shape: {ecg_data.shape}\")\n",
    "        >>> print(f\"Filtered shape: {filtered_ecg.shape}\")\n",
    "\n",
    "    Note:\n",
    "        Uses zero-phase filtering (sosfiltfilt) to avoid phase distortion,\n",
    "        which is critical for maintaining ECG morphology and timing.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    sos = butter(4, [low/nyquist, high/nyquist], btype=\"band\", output=\"sos\")\n",
    "    return sosfiltfilt(sos, signal)\n",
    "\n",
    "def normalize_signal(signal: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize signal to zero mean and unit variance (z-score normalization).\n",
    "\n",
    "    Standardizes the signal by subtracting the mean and dividing by the\n",
    "    standard deviation. This normalization is essential for ECG analysis\n",
    "    as it removes amplitude variations between different leads or recordings\n",
    "    while preserving the relative morphology of cardiac events.\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): Input signal as 1D numpy array to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized signal with mean ≈ 0 and std ≈ 1, same shape\n",
    "            as input.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If signal has zero variance (all values identical).\n",
    "        TypeError: If signal is not a numpy array.\n",
    "\n",
    "    Example:\n",
    "        >>> raw_signal = np.array([1, 2, 3, 4, 5])\n",
    "        >>> normalized = normalize_signal(raw_signal)\n",
    "        >>> print(f\"Original mean: {np.mean(raw_signal):.2f}\")\n",
    "        >>> print(f\"Normalized mean: {np.mean(normalized):.2f}\")\n",
    "        >>> print(f\"Normalized std: {np.std(normalized):.2f}\")\n",
    "\n",
    "    Note:\n",
    "        This implementation assumes the signal is stationary. For long\n",
    "        recordings, consider segmented normalization to handle non-stationarity.\n",
    "    \"\"\"\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "def preprocess_ecg(signal: np.ndarray, fs: int) -> np.ndarray:\n",
    "    \"\"\"Complete ECG preprocessing pipeline: filtering and normalization.\n",
    "\n",
    "    Performs comprehensive preprocessing of ECG signals by applying bandpass\n",
    "    filtering followed by z-score normalization. This two-step process\n",
    "    removes noise artifacts and standardizes signal amplitude, preparing\n",
    "    the data for downstream analysis such as R-peak detection, arrhythmia\n",
    "    classification, or morphology analysis.\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): Raw ECG signal as 1D numpy array.\n",
    "        fs (int): Sampling frequency of the signal in Hz. Common values\n",
    "            are 250, 360, or 500 Hz for clinical ECG recordings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed ECG signal (filtered and normalized) with\n",
    "            same shape as input. Signal will have zero mean, unit variance,\n",
    "            and frequency content limited to 0.5-40 Hz.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If sampling frequency is non-positive or if signal\n",
    "            processing parameters are invalid.\n",
    "        TypeError: If inputs are not of expected types.\n",
    "\n",
    "    Example:\n",
    "        >>> # Simulate noisy ECG signal\n",
    "        >>> t = np.linspace(0, 10, 2500)  # 10 seconds at 250 Hz\n",
    "        >>> ecg_raw = np.sin(2*np.pi*1.2*t) + 0.1*np.random.randn(len(t))\n",
    "        >>> ecg_clean = preprocess_ecg(ecg_raw, fs=250)\n",
    "        >>> print(f\"Raw signal range: [{ecg_raw.min():.2f}, \"\n",
    "        ...       f\"{ecg_raw.max():.2f}]\")\n",
    "        >>> print(f\"Processed signal stats: mean={np.mean(ecg_clean):.3f}, \"\n",
    "        ...       f\"std={np.std(ecg_clean):.3f}\")\n",
    "\n",
    "    Note:\n",
    "        Uses default filter parameters (0.5-40 Hz) optimized for clinical\n",
    "        ECG analysis. For specific applications, consider using filter_ecg()\n",
    "        and normalize_signal() separately with custom parameters.\n",
    "    \"\"\"\n",
    "    filtered = filter_ecg(signal, fs)\n",
    "    return normalize_signal(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa7ff4",
   "metadata": {},
   "source": [
    "### 2. Detekce R vrcholů a výpočet tepové frekvence\n",
    "\n",
    "Funkce `detect_r_peaks` funguje jako digitální detektor srdečního rytmu, který dokáže v EKG záznamu najít přesně vrcholy R vln, které odpovídají hlavní kontrakci srdečních komor. Celý proces probíhá ve čtyřech krocích. Nejprve funkce vypočítá, jak rychle se signál mění z vzorku na vzorek (derivace) a tyto změny umocní na druhou, čímž zvýrazní prudké skoky typické pro QRS komplexy, pak tento \"zostřený\" signál vyhladí pomocí klouzavého průměru přes 80 milisekund, což je [typická doba trvání QRS komplexu](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-interpretation-tutorial/qrs-complex), následně najde všechny vrcholy, které překračují adaptivní práh nastavený na 90. percentil. Tento práh byl opět stanoven iterativně. Začátek byl 98 a postupně snižován, dokud se zlepšovala přesnost detekce. Vrcholy musí být od sebe vzdálené alespoň 300 milisekund, aby se zabránilo falešným detekcím při rychlém srdečním rytmu. Autor uvažuje maximální tepovou frekvenci 200 tepů za minutu, což může být nedostatečné v případě závažných srdečních onemocnění nebo extrémních sportovních výkonů. Pokud se nejedná o žádnou z výše uvedených, jedná se o anomálii. Pro potřeby práce s databázemi BUTQDB a NSRDB je ovšem postačující. Stejně tak by měla být postačující pro práci s dlouhodobými měřeními typu Holter a podobným post hoc zpracováním. Detekce se nakonec vrátí k původnímu signálu a kolem každého detekovaného místa v okně 50 milisekund najde skutečný nejvyšší bod, který představuje přesnou polohu R vrcholu. Celý tento postup je inspirovaný slavným Pan-Tompkinsovým algoritmem a je navržen tak, aby spolehlivě rozpoznal srdeční rytmus i v zašuměných záznamech, přičemž dokáže pracovat s různými tvary EKG signálů a frekvencemi vzorkování.\n",
    "\n",
    "Funkce `calculate_bpm` sleduje, jak se tepová frekvence mění v průběhu času během celého EKG záznamu. Celý záznam je rozdělen na stejně dlouhé časové úseky (obvykle po 60 sekundách) a v každém z těchto úseků je vypočteno, kolik srdečních úderů se odehrálo. Funkce postupně prochází každé časové okno, najde všechny R vrcholy (které jsou detekované v předešlém kroku) spadající do daného intervalu, spočítá je a přepočítá na standardní jednotku tepů za minutu pomocí jednoduchého vzorce: počet úderů krát 60 děleno délkou okna v sekundách. Výsledkem je pak časová řada hodnot tepové frekvence, kde každá hodnota odpovídá začátku příslušného časového okna, což umožňuje sledovat, jak se srdeční rytmus vyvíjel během celého vyšetření. Tento přístup je užitečný pro analýzu variability srdeční frekvence a pro detekci abnormalit, které se mohou projevit postupně v čase, nikoli jen v jednotlivých srdečních cyklech. V této práci je funkce použita pouze k výpočtu průměrné tepové frekvence v průběhu celého měření. To může být dále využito k monitorování fyzické zdatnosti, hladiny stresu a dalších."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aedbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  2. R-PEAKS DETECTION AND BPM COMPUTATION\n",
    "# =============================================================================\n",
    "def detect_r_peaks(signal: np.ndarray, fs: int) -> np.ndarray:\n",
    "    \"\"\"Detect R-peaks using Pan-Tompkins inspired algorithm.\n",
    "\n",
    "    Implements a modified Pan-Tompkins algorithm for QRS complex detection\n",
    "    in ECG signals. The algorithm uses differentiation, squaring, and moving\n",
    "    average integration to enhance QRS complexes, followed by adaptive\n",
    "    thresholding and peak refinement to accurately locate R-peaks.\n",
    "\n",
    "    The algorithm consists of four main steps:\n",
    "    1. Differentiation and squaring to emphasize slope changes\n",
    "    2. Moving average integration to smooth the signal\n",
    "    3. Adaptive thresholding with minimum distance constraints\n",
    "    4. Peak refinement in the original signal domain\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): Preprocessed ECG signal as 1D numpy array.\n",
    "            Should be filtered and normalized for optimal performance.\n",
    "        fs (int): Sampling frequency in Hz. Common values are 250, 360,\n",
    "            or 500 Hz for clinical ECG recordings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of R-peak indices (sample positions) in the\n",
    "            original signal. Returns empty array if no peaks detected.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If sampling frequency is non-positive or signal is empty.\n",
    "        TypeError: If inputs are not of expected types.\n",
    "\n",
    "    Example:\n",
    "        >>> # Detect R-peaks in a 10-second ECG recording\n",
    "        >>> ecg_signal = preprocess_ecg(raw_ecg, fs=250)\n",
    "        >>> r_peaks = detect_r_peaks(ecg_signal, fs=250)\n",
    "        >>> print(f\"Detected {len(r_peaks)} R-peaks\")\n",
    "        >>> # Convert to time domain\n",
    "        >>> peak_times = r_peaks / 250.0  # seconds\n",
    "        >>> print(f\"First peak at {peak_times[0]:.2f} seconds\")\n",
    "\n",
    "    Note:\n",
    "        - Uses 80ms integration window (typical QRS duration: 60-120ms)\n",
    "        - Minimum peak distance of 300ms prevents double detection\n",
    "        - Peak refinement searches ±50ms around detected locations\n",
    "        - Threshold set at 90th percentile for adaptive performance\n",
    "        - Algorithm assumes heart rate between 20-200 BPM\n",
    "    \"\"\"\n",
    "    # Differentiate and square to emphasize QRS slope changes\n",
    "    diff = np.ediff1d(signal, to_end=0)\n",
    "    squared = diff ** 2\n",
    "\n",
    "    # Moving average integration to smooth and enhance QRS complexes\n",
    "    window_size = int(0.08 * fs)  # 80ms window for QRS enhancement\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    integrated = np.convolve(squared, kernel, mode=\"same\")\n",
    "\n",
    "    # Find peaks with adaptive threshold and minimum distance constraint\n",
    "    threshold = np.percentile(integrated, 90)  # Adaptive threshold\n",
    "    min_distance = int(0.3 * fs)  # 300ms minimum (prevents >200 BPM)\n",
    "\n",
    "    peaks, _ = find_peaks(integrated, height=threshold,\n",
    "                          distance=min_distance)\n",
    "\n",
    "    # Refine peak locations in original signal for accurate R-peak timing\n",
    "    r_peaks = []\n",
    "    search_window = int(0.05 * fs)  # 50ms search window around detection\n",
    "\n",
    "    for peak in peaks:\n",
    "        start = max(0, peak - search_window)\n",
    "        end = min(len(signal), peak + search_window)\n",
    "        if start < end:\n",
    "            # Find maximum absolute value (handles inverted R-waves)\n",
    "            local_max = np.argmax(np.abs(signal[start:end])) + start\n",
    "            r_peaks.append(local_max)\n",
    "\n",
    "    return np.array(r_peaks)\n",
    "\n",
    "def calculate_bpm(r_peaks: np.ndarray, fs: int, duration_sec: float,\n",
    "                  window_sec: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calculate heart rate (BPM) in sliding time windows.\n",
    "\n",
    "    Computes instantaneous heart rate by counting R-peaks within sliding\n",
    "    time windows. This approach provides time-resolved heart rate analysis\n",
    "    suitable for detecting heart rate variability and trends over time.\n",
    "\n",
    "    Args:\n",
    "        r_peaks (np.ndarray): Array of R-peak indices from detect_r_peaks().\n",
    "        fs (int): Sampling frequency in Hz used for time conversion.\n",
    "        duration_sec (float): Total duration of the ECG recording in seconds.\n",
    "        window_sec (int, optional): Window size for BPM calculation in\n",
    "            seconds. Defaults to 60. Smaller windows provide higher\n",
    "            temporal resolution but may be less stable.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
    "            - bpm_values: Array of heart rate values in beats per minute\n",
    "            - time_minutes: Array of time points in minutes corresponding\n",
    "              to each BPM value (window centers)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If duration_sec is negative or window_sec is non-positive.\n",
    "        TypeError: If inputs are not of expected types.\n",
    "\n",
    "    Example:\n",
    "        >>> # Calculate BPM for 5-minute ECG recording\n",
    "        >>> r_peaks = detect_r_peaks(ecg_signal, fs=250)\n",
    "        >>> bpm, time_points = calculate_bpm(r_peaks, fs=250,\n",
    "        ...                                  duration_sec=300, window_sec=60)\n",
    "        >>> print(f\"Average heart rate: {np.mean(bpm):.1f} BPM\")\n",
    "        >>> print(f\"Heart rate range: {np.min(bpm):.1f}-{np.max(bpm):.1f}\")\n",
    "        >>>\n",
    "        >>> # Plot heart rate over time\n",
    "        >>> import matplotlib.pyplot as plt\n",
    "        >>> plt.plot(time_points, bpm)\n",
    "        >>> plt.xlabel(\"Time (minutes)\")\n",
    "        >>> plt.ylabel(\"Heart Rate (BPM)\")\n",
    "\n",
    "    Note:\n",
    "        - Returns empty arrays if no R-peaks provided\n",
    "        - BPM calculated as: (peaks_in_window * 60) / window_duration\n",
    "        - Windows that extend beyond recording duration are truncated\n",
    "        - For recordings shorter than window_sec, consider using smaller\n",
    "          window sizes or instantaneous heart rate calculations\n",
    "        - Time points represent the start of each window\n",
    "    \"\"\"\n",
    "    if len(r_peaks) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Calculate number of complete windows that fit in the duration\n",
    "    n_windows = int(duration_sec // window_sec)\n",
    "    if n_windows == 0:\n",
    "        # Handle case where duration is shorter than window size\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    bpm_values = np.zeros(n_windows)\n",
    "\n",
    "    # Calculate BPM for each window\n",
    "    for i in range(n_windows):\n",
    "        start_sample = i * window_sec * fs\n",
    "        end_sample = (i + 1) * window_sec * fs\n",
    "\n",
    "        # Count R-peaks within current window\n",
    "        peaks_in_window = np.sum((r_peaks >= start_sample) &\n",
    "                                 (r_peaks < end_sample))\n",
    "\n",
    "        # Convert count to beats per minute\n",
    "        bpm_values[i] = peaks_in_window * 60 / window_sec\n",
    "\n",
    "    # Create time axis in minutes (start time of each window)\n",
    "    time_minutes = np.arange(n_windows) * (window_sec / 60)\n",
    "\n",
    "    return bpm_values, time_minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cda72",
   "metadata": {},
   "source": [
    "### 3. Zpracování jednotlivých souborů\n",
    "\n",
    "Tento kód představuje kompletní systém pro automatickou analýzu EKG záznamů schopný zpracovat celé soubory s EKG daty a vyhodnotit srdeční aktivitu. Celý proces začíná načtením EKG souboru ve standardním WFDB formátu. Systém nejprve přečte hlavičku souboru, aby zjistil základní parametry jako vzorkovací frekvenci, a pak načte samotný signál, přičemž automaticky vybere standardní Lead II (druhý svod), který je preferovaný kvůli tomu, že signál ve druhém svodu je standardně silnější a čitelnější, nebo použije první dostupný svod, pokud Lead II není k dispozici. Následně systém zpracuje celý načtený signál pomocí dříve popsaných funkcí - nejprve jej vyčistí a normalizuje, pak najde všechny R vrcholy a spočítá tepovou frekvenci v čase pomocí posuvného okna o délce jedné minuty. Když je požadováno grafické znázornění vývoje tepové frekvence v průběhu měření, systém vytvoří graf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  3. SINGLE FILE ECG PROCESSING\n",
    "# =============================================================================\n",
    "def load_ecg_file(file_path: str,\n",
    "                  max_duration_hours: float | None = None) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Load ECG file from WFDB format with optional duration limiting.\n",
    "\n",
    "    Loads ECG data from WFDB (WaveForm DataBase) format files, which are\n",
    "    commonly used for storing physiological signals. Supports duration\n",
    "    limiting for memory efficiency when processing long recordings.\n",
    "    Automatically selects Lead II if available (standard for arrhythmia\n",
    "    analysis), otherwise uses the first available lead.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the WFDB record file (without extension).\n",
    "            The function expects both .hea (header) and .dat (data) files\n",
    "            to be present with this base name.\n",
    "        max_duration_hours (float | None, optional): Maximum duration to\n",
    "            load in hours. If None, loads entire recording. Useful for\n",
    "            limiting memory usage with long recordings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, int]: A tuple containing:\n",
    "            - signal: ECG signal as 1D numpy array (Lead II or first lead)\n",
    "            - fs: Sampling frequency in Hz from the header file\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If WFDB files (.hea or .dat) are not found.\n",
    "        ValueError: If max_duration_hours is negative.\n",
    "        Exception: For other WFDB reading errors or corrupted files.\n",
    "\n",
    "    Example:\n",
    "        >>> # Load complete ECG recording\n",
    "        >>> signal, fs = load_ecg_file(\"data/patient001\")\n",
    "        >>> print(f\"Loaded {len(signal)} samples at {fs} Hz\")\n",
    "        >>>\n",
    "        >>> # Load only first 2 hours to save memory\n",
    "        >>> signal, fs = load_ecg_file(\"data/longrecord\",\n",
    "        ...                            max_duration_hours=2.0)\n",
    "        >>> duration = len(signal) / fs / 3600  # Convert to hours\n",
    "        >>> print(f\"Loaded {duration:.1f} hours of data\")\n",
    "\n",
    "    Note:\n",
    "        - Prioritizes Lead II (index 1) for standard arrhythmia analysis\n",
    "        - Falls back to first lead if Lead II unavailable\n",
    "        - Memory usage scales with duration; use max_duration_hours for\n",
    "          long recordings\n",
    "        - WFDB format expects both .hea (header) and .dat (data) files\n",
    "        - Sampling frequency is read from header file\n",
    "    \"\"\"\n",
    "    # Read header to get recording parameters\n",
    "    header = wfdb.rdheader(file_path)\n",
    "    fs = header.fs\n",
    "\n",
    "    # Calculate samples to read based on duration limit\n",
    "    if max_duration_hours:\n",
    "        max_samples = int(max_duration_hours * 3600 * fs)\n",
    "        sampto = min(header.sig_len, max_samples)\n",
    "    else:\n",
    "        sampto = header.sig_len\n",
    "\n",
    "    # Read the actual signal data\n",
    "    record = wfdb.rdrecord(file_path, sampto=sampto)\n",
    "\n",
    "    # Extract signal: prioritize Lead II, fallback to first available lead\n",
    "    if len(record.p_signal.shape) > 1:\n",
    "        # Multi-lead recording\n",
    "        if record.p_signal.shape[1] >= 2:\n",
    "            signal = record.p_signal[:, 1]  # Lead II (index 1)\n",
    "        else:\n",
    "            signal = record.p_signal[:, 0]  # First lead only\n",
    "    else:\n",
    "        # Single lead recording\n",
    "        signal = record.p_signal.flatten()  # Ensure 1D array\n",
    "\n",
    "    return signal, fs\n",
    "\n",
    "\n",
    "def analyze_ecg_file(data_dir: str, record_name: str, plot: bool = True,\n",
    "                     max_duration_hours: float | None = None) -> Dict:\n",
    "    \"\"\"Analyze single ECG file for R-peak detection and heart rate analysis.\n",
    "\n",
    "    Performs complete ECG analysis pipeline including file loading,\n",
    "    preprocessing, R-peak detection, and heart rate calculation. Optionally\n",
    "    generates time-series plots of heart rate trends. Designed for batch\n",
    "    processing of multiple ECG recordings with consistent error handling.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory containing WFDB ECG files.\n",
    "        record_name (str): Base name of the WFDB record (without extension).\n",
    "            Function will look for {record_name}_ECG.hea and\n",
    "            {record_name}_ECG.dat files.\n",
    "        plot (bool, optional): Whether to display heart rate plot.\n",
    "            Defaults to True. Set to False for batch processing.\n",
    "        max_duration_hours (float | None, optional): Maximum duration to\n",
    "            analyze in hours. If None, analyzes entire recording.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Analysis results dictionary containing:\n",
    "            - record_name (str): Name of processed record\n",
    "            - success (bool): Whether analysis completed successfully\n",
    "            - fs (int): Sampling frequency in Hz\n",
    "            - total_samples (int): Number of samples processed\n",
    "            - duration_hours (float): Recording duration in hours\n",
    "            - r_peaks_detected (int): Number of R-peaks found\n",
    "            - average_bpm (float): Mean heart rate in BPM\n",
    "            - bpm_values (np.ndarray): Heart rate time series\n",
    "            - time_minutes (np.ndarray): Time points for BPM values\n",
    "            - error (str): Error message if success=False\n",
    "\n",
    "    Raises:\n",
    "        No exceptions raised - errors are caught and returned in result dict.\n",
    "\n",
    "    Example:\n",
    "        >>> # Analyze single file with plotting\n",
    "        >>> result = analyze_ecg_file(\"./data\", \"patient_001\", plot=True)\n",
    "        >>> if result[\"success\"]:\n",
    "        ...     print(f\"Average HR: {result[\"average_bpm\"]:.1f} BPM\")\n",
    "        ...     print(f\"Duration: {result[\"duration_hours\"]:.1f} hours\")\n",
    "        >>>\n",
    "        >>> # Batch processing without plots\n",
    "        >>> records = [\"patient_001\", \"patient_002\", \"patient_003\"]\n",
    "        >>> results = []\n",
    "        >>> for record in records:\n",
    "        ...     result = analyze_ecg_file(\"./data\", record, plot=False,\n",
    "        ...                               max_duration_hours=1.0)\n",
    "        ...     results.append(result)\n",
    "\n",
    "    Note:\n",
    "        - Uses garbage collection to manage memory in batch processing\n",
    "        - Prints progress information to console\n",
    "        - Handles all exceptions gracefully with error reporting\n",
    "        - Expected file naming: {record_name}_ECG.hea/.dat\n",
    "        - Plot displays if plot=True and analysis successful\n",
    "        - BPM calculated in 60-second sliding windows by default\n",
    "    \"\"\"\n",
    "    # Construct file path with expected naming convention\n",
    "    file_path = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "\n",
    "    try:\n",
    "        # Load and preprocess ECG signal\n",
    "        signal, fs = load_ecg_file(file_path, max_duration_hours)\n",
    "        processed_signal = preprocess_ecg(signal, fs)\n",
    "\n",
    "        # Detect R-peaks using Pan-Tompkins algorithm\n",
    "        r_peaks = detect_r_peaks(processed_signal, fs)\n",
    "\n",
    "        # Calculate heart rate in sliding windows\n",
    "        duration_sec = len(signal) / fs\n",
    "        bpm_values, time_minutes = calculate_bpm(r_peaks, fs, duration_sec)\n",
    "        avg_bpm = np.mean(bpm_values) if len(bpm_values) > 0 else 0\n",
    "\n",
    "        # Generate heart rate plot if requested and data available\n",
    "        if plot and len(bpm_values) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(time_minutes, bpm_values, \"-\", linewidth=1)\n",
    "            plt.title(f\"Heart Rate Over Time - {record_name}\")\n",
    "            plt.xlabel(\"Time (minutes)\")\n",
    "            plt.ylabel(\"Heart Rate (BPM)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Compile analysis results\n",
    "        result = {\n",
    "            \"record_name\": record_name,\n",
    "            \"success\": True,\n",
    "            \"fs\": fs,\n",
    "            \"total_samples\": len(signal),\n",
    "            \"duration_hours\": duration_sec / 3600,\n",
    "            \"r_peaks_detected\": len(r_peaks),\n",
    "            \"average_bpm\": avg_bpm,\n",
    "            \"bpm_values\": bpm_values,\n",
    "            \"time_minutes\": time_minutes\n",
    "        }\n",
    "\n",
    "        # Print progress information\n",
    "        print(f\"Processed {record_name}: {len(r_peaks)} R-peaks, \"\n",
    "              f\"{avg_bpm:.1f} BPM avg\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors gracefully and return error information\n",
    "        error_msg = f\"Error processing {record_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            \"record_name\": record_name,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Clean up memory after processing (important for batch processing)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4e349",
   "metadata": {},
   "source": [
    "### 4. Dávkové zpracování\n",
    "\n",
    "Systém nejprve prohledá zadanou složku a najde všechny soubory s EKG záznamy (rozpozná je podle koncovky \"_ECG.hea\"), pak je seřadí podle abecedy, aby bylo zpracování konzistentní a reprodukovatelné. Následně každý soubor postupně předá ke kompletnímu zpracování výše uvedenými funkcemi. Systém celý čas sleduje průběh a vypisuje informace o tom, kolik souborů už zpracoval. Pokud se nějaký soubor nepodaří zpracovat (třeba je poškozený nebo má nesprávný formát), nevypne celou analýzu, ale jen tento problematický soubor přeskočí, vypíše chybovou zprávu a pokračuje dál. Na konci celého procesu všechny úspěšně zpracované výsledky poskládá do přehledné tabulky, kde každý řádek reprezentuje jeden záznam a sloupce obsahují klíčové informace jako jméno záznamu, vzorkovací frekvenci, délku záznamu, počet detekovaných srdečních úderů a průměrnou tepovou frekvenci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  4. BATCH ECG PROCESSING\n",
    "# =============================================================================\n",
    "def analyze_ecg_batch(data_dir: str, max_files: int | None = None,\n",
    "                      max_duration_hours: float | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Process multiple ECG files in a directory for batch analysis.\n",
    "\n",
    "    Performs automated batch processing of ECG recordings in WFDB format,\n",
    "    generating a summary DataFrame with key metrics for each successfully\n",
    "    processed file. Designed for efficient analysis of large ECG datasets\n",
    "    with memory management and progress tracking.\n",
    "\n",
    "    The function searches for files matching the pattern \"*_ECG.hea\" in the\n",
    "    specified directory and processes each one through the complete ECG\n",
    "    analysis pipeline (loading, preprocessing, R-peak detection, and heart\n",
    "    rate calculation).\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to directory containing WFDB ECG files.\n",
    "            Must contain files with naming pattern \"{record_name}_ECG.hea\"\n",
    "            and corresponding \"{record_name}_ECG.dat\" files.\n",
    "        max_files (int | None, optional): Maximum number of files to process.\n",
    "            If None, processes all available files. Useful for testing or\n",
    "            limiting computational load. Files are processed in alphabetical\n",
    "            order. Defaults to None.\n",
    "        max_duration_hours (float | None, optional): Maximum duration per\n",
    "            file to analyze in hours. If None, processes entire recordings.\n",
    "            Helps manage memory usage for long recordings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary DataFrame with columns:\n",
    "            - Record: Record name (string)\n",
    "            - Sampling_Rate_Hz: Sampling frequency in Hz (int)\n",
    "            - Duration_Hours: Recording duration in hours (float)\n",
    "            - R_Peaks: Number of detected R-peaks (int)\n",
    "            - Average_BPM: Mean heart rate in BPM (float)\n",
    "            Returns empty DataFrame if no files processed successfully.\n",
    "\n",
    "    Raises:\n",
    "        No exceptions raised - errors are handled gracefully with console\n",
    "        output and skipped files.\n",
    "\n",
    "    Example:\n",
    "        >>> # Process all ECG files in directory\n",
    "        >>> results = analyze_ecg_batch(\"./ecg_data\")\n",
    "        >>> print(f\"Processed {len(results)} files successfully\")\n",
    "        >>> print(f\"Average heart rate: {results[\"Average_BPM\"].mean():.1f}\")\n",
    "        >>>\n",
    "        >>> # Process only first 10 files, limit to 2 hours each\n",
    "        >>> results = analyze_ecg_batch(\"./ecg_data\", max_files=10,\n",
    "        ...                             max_duration_hours=2.0)\n",
    "        >>>\n",
    "        >>> # Save results to CSV\n",
    "        >>> results.to_csv(\"ecg_analysis_results.csv\", index=False)\n",
    "        >>>\n",
    "        >>> # Display summary statistics\n",
    "        >>> print(results.describe())\n",
    "\n",
    "    Note:\n",
    "        - Files processed in alphabetical order by record name\n",
    "        - Progress printed to console during processing\n",
    "        - Failed files are skipped with error messages\n",
    "        - Memory cleanup performed after each file\n",
    "        - Only successfully processed files included in results\n",
    "        - Plotting disabled for batch processing efficiency\n",
    "        - Expected file naming: {record_name}_ECG.hea/.dat\n",
    "    \"\"\"\n",
    "    # Validate input directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find all ECG header files in directory\n",
    "    ecg_files = [f for f in os.listdir(data_dir)\n",
    "                 if f.endswith(\"_ECG.hea\")]\n",
    "\n",
    "    # Extract record names and sort alphabetically for consistent processing\n",
    "    record_names = sorted([f.replace(\"_ECG.hea\", \"\") for f in ecg_files])\n",
    "\n",
    "    # Limit number of files if specified\n",
    "    if max_files:\n",
    "        record_names = record_names[:max_files]\n",
    "\n",
    "    print(f\"Processing {len(record_names)} ECG files from {data_dir}\")\n",
    "\n",
    "    # Process each file and collect results\n",
    "    results = []\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        print(f\"File {i+1}/{len(record_names)}: {record_name}\")\n",
    "\n",
    "        # Analyze individual ECG file (plotting disabled for batch mode)\n",
    "        result = analyze_ecg_file(data_dir, record_name, plot=False,\n",
    "                                  max_duration_hours=max_duration_hours)\n",
    "\n",
    "        # Add successful results to summary\n",
    "        if result.get(\"success\", False):\n",
    "            results.append({\n",
    "                \"Record\": result[\"record_name\"],\n",
    "                \"Sampling_Rate_Hz\": result[\"fs\"],\n",
    "                \"Duration_Hours\": result[\"duration_hours\"],\n",
    "                \"R_Peaks\": result[\"r_peaks_detected\"],\n",
    "                \"Average_BPM\": result[\"average_bpm\"]\n",
    "            })\n",
    "\n",
    "        # Clean up memory after each file (important for large datasets)\n",
    "        gc.collect()\n",
    "\n",
    "    # Create and return summary DataFrame\n",
    "    return pd.DataFrame(results) if results else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b0146",
   "metadata": {},
   "source": [
    "### 5. Vyhodnocení detekce\n",
    "\n",
    "Níže uvedený kód vyhodnocuje úspěšnost vytvořeného algoritmu na správných anotovaných datech z databáze NSRDB. Celý proces začíná tím, že systém načte jak EKG signál, tak soubor s anotacemi, který obsahuje přesné pozice všech skutečných R vrcholů. Tyto anotace slouží jako \"zlatý standard\" pro porovnání. Poté systém spustí detektor R vrcholů na stejném signálu a získá seznam pozic, které algoritmus vyhodnotí jako srdeční údery. Následuje párování detekovaných vrcholů se skutečnými vrcholy v mezích stanovené tolerance. Je nezbytné, aby přípustná odchylka byla nižší než 300 ms stanovených při určování R vrcholů. V tomto případě byla stanovena polovina intervalu, což znamená, že pokud algoritmus najde vrchol do 150 ms od skutečného vrcholu, počítá se to jako správná detekce, protože malé časové odchylky jsou v pořádku. Na základě tohoto párování systém spočítá tři klíčové statistiky: preciznost (kolik z detekovaných vrcholů bylo skutečně správných), recall neboli citlivost (kolik ze skutečných vrcholů se podařilo najít) a F1-score (harmonický průměr obou předchozích metrik, který dává celkové hodnocení kvality). Verze této funkce pro hromadné zpracování souborů pak umožňuje otestovat algoritmus na desítkách různých EKG záznamů najednou a vytvořit kompletní statistický přehled výkonnosti.\n",
    "\n",
    "## Detekce anomálií v signálech\n",
    "\n",
    "### Zadání druhé části\n",
    "\n",
    "Ve zdrojové databázi najdete celkem 18 měření EKG obsahující úplné (3 signály) nebo částečné\n",
    "anotace událostí (P,T vlny a QRS komplex). Záznamy EKg obsahují i části, které jsou porušeny\n",
    "vlivem anomálií (vnější rušení, manipulace s pacientem apod.). Navrhněte způsob, jak\n",
    "detekovat tyto úseky a prezentujte statistiku výskytu úseků v měřeních.\n",
    "\n",
    "### Obecný postup řešení\n",
    "\n",
    "Narozdíl od předchozí části je signál po načtení pouze normalizován, aby nedošlo k odstranění anomálií. Následně jsou současně aplikovány tři různé detekční metody. První hledá neobvykle vysoké nebo nízké amplitudy, druhá identifikuje \"ploché\" úseky s minimální variabilitou, které mohou signalizovat odpojený senzor, a třetí detekuje nadměrný šum. Výsledky všech tří metod se poté kombinují do jednotné masky anomálií, která se následně čistí spojováním blízkých detekovaných úseků a odfiltrováním příliš krátkých segmentů, aby se minimalizovaly falešné poplachy a fragmentace. Systém nakonec porovnává své výsledky s expertně anotovanými referenčními daty pro vyhodnocení přesnosti a vytváří vizualizaci s barevně odlišenými anomáliemi, přičemž celý tento přístup je navržen tak, aby kombinoval různé typy detekce pro maximální pokrytí možných problémů v EKG záznamech.\n",
    "\n",
    "Zásadním problémem této části je neexistence anotovaných záznamů s uvedeným výskytem anomálií a z toho vycházející nemožnost vyhodnocení detekce anomálií. V abstraktu BUTQDB je uvedeno, že *\"Three signals were fully annotated in terms of ECG signal quality. The remaining 15 signals were annotated in two selected segments, each of 20 minutes in duration. Furthermore, five additional segments of poor signal quality were also annotated. Signal quality was classified as follows: Class 1 indicates that all ECG significant waveforms (P wave, T wave and QRS complex) are clearly visible and their onsets and offsets can be detected reliably; Class 2 indicates that the noise level is increased and ECG significant points cannot be reliably detected, but the signal enables reliable QRS detection; Class 3 indicates that QRS complexes cannot be detected reliably and the signal is unsuitable for any further analysis.\"* Z výše uvedeného ovšem není jasný počet abnormalit v daném úseku. Stejně tak úseky označené jedničkou jsou dobře rozpoznatelné, ale to samotné neznačí naprostou absenci abnormalit.\n",
    "\n",
    "Kvůli tomuto byly za potvrzené anomálie označeny úseky anotované 2 a 3. Za anomálii je označen vždy celý úsek. Pokud se detekovaný abnormální úsek překrývá s potvrzeným abnormálním úsekem alespoň 10 %, je považován za skutečně pozitivní. Absence anotovaných úseků tak vede k vysokému počtu falešně pozitivních výsledků a nemožnosti výpočtu relevantní statistiky. Toto vede ke gamifikaci a optimalizaci parametrů pro maximalizaci skutečně pozitivních výsledků. Ačkoliv tato není předmětem této práce, je ponechána na konci pro zajímavost.\n",
    "\n",
    "### Popis zvolených metod\n",
    "\n",
    "Systém začne tím, že si načte EKG signál a nastaví parametry pro detekci. Tyto parametry určují citlivost detektoru. Z výše uvedených důvodů jsou nastaveny na maximalizaci nalezení skutečně pozitivních úseků. Kód využívá tři různé metody detekce, protože každá dokáže najít jiný typ problému. Metoda `detect_amplitude_anomalies` hledá místa, kde je signál neobvykle vysoký nebo nízký. Funguje tak, že spočítá průměr a rozptyl celého signálu, a pak označí za anomálie všechna místa, kde se hodnota výrazně liší od normálu.\n",
    "\n",
    "Metoda `detect_flatline_anomalies` hledá úseky, kde se signál téměř nemění - což může znamenat, že se odpojil sensor nebo je jiný technický problém. Používá \"klouzavé okno\". Postupně se dívá na malé úseky signálu a počítá, jak moc se v tom úseku hodnoty mění. Pokud se téměř nemění, označí to za anomálii.\n",
    "\n",
    "Metoda `detect_noise_anomalies` hledá místa s příliš mnoha rychlými změnami, které mohou být způsobené elektrickým rušením nebo pohybem pacienta. Počítá derivaci signálu a hledá místa, kde jsou tyto změny neobvykle velké.\n",
    "\n",
    "Výběr těchto tří metod dává smysl z medicínského hlediska. Amplitudové anomálie zachytí problémy s úrovní signálu. Flatline detekce najde technické výpadky.Detekce šumu identifikuje rušení a artefakty. Společně pokrývají většinu běžných problémů, které se v EKG záznamech vyskytují.\n",
    "\n",
    "Po detekci systém výsledky zpracuje. Blízké anomálie jsou spojeny. Pokud jsou dvě anomálie blízko u sebe, jsou spojeny do jedné, aby se zabránilo fragmentaci. Příliš krátké úseky jsou odfiltrovány, protože velmi krátké anomálie často nejsou významné. Vytvoří se jednotný seznam všech problematických úseků. Seznam je následně porovnán s referenčními daty. Výsledkem je statistika nalezených skutečně pozitivních, falešně pozitivních a falešně negativních. Kvůli uvedeným problémům ovšem nelze statistice přikládat příliš velkou váhu. Na závěr je vytvořen graf znázorňující přesah anotovaných a nalezených úseků. Graf ovšem trpí stejnými vadami jako předešlá statistika úspěšnosti.\n",
    "\n",
    "Kombinace tří různých metod je efektivní, protože každá metoda cílí na jiný typ problému. Společně zachytí více situací než každá zvlášť a tvoří tak robustní systém. Tento ovšem není bezchybný a nabízí prostor pro další metody, například detekci arytmií. Tyto problémy ovšem databáze dle jejich popisu neobsahují, proto není žádoucí jejich detekci do systému přidávat.\n",
    "\n",
    "Parametry jako k_std, flat_duration_sec a noise_multiplier fungují jako citlivost každého detektoru. Každý parametr lze nastavit dle požadované přísnosti detekce. S těmito daty je incentiva k použití benevolentních hodnot a maximální detekci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a619395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  6. ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AnomalyResults:\n",
    "    \"\"\"Data class to store anomaly detection results.\n",
    "\n",
    "    This class encapsulates all the results from an anomaly detection run,\n",
    "    including detected intervals, ground truth comparisons, and evaluation\n",
    "    metrics.\n",
    "\n",
    "    Attributes:\n",
    "        detected_intervals: List of (start, end) tuples for detected anomalies.\n",
    "        gt_intervals: List of (start, end) tuples for ground truth intervals.\n",
    "        total_gt: Total number of ground truth intervals.\n",
    "        total_detected: Total number of detected intervals.\n",
    "        true_positives: Number of correctly detected anomalies.\n",
    "        accuracy: Fraction of ground truth intervals that were detected.\n",
    "        detection_methods: List of detection methods used.\n",
    "        signal_length: Length of the analyzed signal in samples.\n",
    "    \"\"\"\n",
    "    detected_intervals: List[Tuple[int, int]]\n",
    "    gt_intervals: List[Tuple[int, int]]\n",
    "    total_gt: int\n",
    "    total_detected: int\n",
    "    true_positives: int\n",
    "    accuracy: float\n",
    "    detection_methods: List[str]\n",
    "    signal_length: int\n",
    "\n",
    "\n",
    "class ECGAnomalyDetector:\n",
    "    \"\"\"ECG Anomaly Detection and Evaluation Pipeline.\n",
    "\n",
    "    This class implements multiple anomaly detection methods for ECG signals\n",
    "    including amplitude-based, flatline, and noise detection. It provides\n",
    "    a unified interface for detecting anomalies and post-processing the\n",
    "    results.\n",
    "\n",
    "    The detector uses configurable thresholds and windows to adapt to\n",
    "    different signal characteristics and anomaly types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 k_std: float = 1.5,\n",
    "                 flat_duration_sec: float = 2.0,\n",
    "                 noise_duration_sec: float = 0.5,\n",
    "                 noise_multiplier: float = 1.0,\n",
    "                 min_segment_length: int = 50,\n",
    "                 merge_gap: int = 50):\n",
    "        \"\"\"Initialize anomaly detector with parameters.\n",
    "\n",
    "        Args:\n",
    "            k_std: Standard deviation threshold for amplitude anomalies.\n",
    "                Values above mean ± k_std * std are considered anomalous.\n",
    "            flat_duration_sec: Duration in seconds for flatline detection\n",
    "                window. Longer windows are more sensitive to sustained\n",
    "                flatlines.\n",
    "            noise_duration_sec: Duration in seconds for noise detection\n",
    "                window. Shorter windows detect more transient noise.\n",
    "            noise_multiplier: Multiplier for noise threshold. Higher values\n",
    "                reduce sensitivity to noise.\n",
    "            min_segment_length: Minimum length in samples for anomaly\n",
    "                segments. Shorter segments are filtered out.\n",
    "            merge_gap: Maximum gap in samples to merge adjacent segments.\n",
    "                Segments closer than this gap are merged.\n",
    "        \"\"\"\n",
    "        self.k_std = k_std\n",
    "        self.flat_duration_sec = flat_duration_sec\n",
    "        self.noise_duration_sec = noise_duration_sec\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.min_segment_length = min_segment_length\n",
    "        self.merge_gap = merge_gap\n",
    "\n",
    "    def _rolling_std(self, signal: np.ndarray, window: int) -> np.ndarray:\n",
    "        \"\"\"Calculate rolling standard deviation efficiently.\n",
    "\n",
    "        Uses convolution-based approach for computational efficiency on\n",
    "        large signals.\n",
    "\n",
    "        Args:\n",
    "            signal: Input signal array.\n",
    "            window: Window size in samples for rolling calculation.\n",
    "\n",
    "        Returns:\n",
    "            Array of rolling standard deviation values, same length as input.\n",
    "        \"\"\"\n",
    "        m = uniform_filter1d(signal, window, mode=\"nearest\")\n",
    "        sq = uniform_filter1d(signal * signal, window, mode=\"nearest\")\n",
    "        return np.sqrt(np.maximum(sq - m * m, 0))\n",
    "\n",
    "    def _mask_to_intervals(self, mask: np.ndarray) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Convert boolean mask to list of (start, end) intervals.\n",
    "\n",
    "        Finds contiguous regions of True values in the mask and converts\n",
    "        them to interval tuples.\n",
    "\n",
    "        Args:\n",
    "            mask: Boolean array where True indicates anomalous samples.\n",
    "\n",
    "        Returns:\n",
    "            List of (start, end) tuples representing anomalous intervals.\n",
    "            End indices are inclusive.\n",
    "        \"\"\"\n",
    "        # Find transitions\n",
    "        diff = np.diff(np.r_[0, mask.astype(int), 0])\n",
    "        starts = np.flatnonzero(diff == 1)\n",
    "        ends = np.flatnonzero(diff == -1) - 1\n",
    "        return list(zip(starts, ends))\n",
    "\n",
    "    def _merge_intervals(self,\n",
    "                        intervals: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Merge intervals that are close together.\n",
    "\n",
    "        Intervals separated by less than merge_gap samples are combined\n",
    "        into single intervals to reduce fragmentation.\n",
    "\n",
    "        Args:\n",
    "            intervals: List of (start, end) interval tuples to merge.\n",
    "\n",
    "        Returns:\n",
    "            List of merged intervals, sorted by start position.\n",
    "        \"\"\"\n",
    "        if not intervals:\n",
    "            return []\n",
    "\n",
    "        # Sort intervals by start position\n",
    "        intervals = sorted(intervals)\n",
    "        merged = [intervals[0]]\n",
    "\n",
    "        for start, end in intervals[1:]:\n",
    "            last_start, last_end = merged[-1]\n",
    "\n",
    "            # If current interval is within merge_gap of last interval\n",
    "            if start - last_end <= self.merge_gap:\n",
    "                # Merge intervals\n",
    "                merged[-1] = (last_start, max(last_end, end))\n",
    "            else:\n",
    "                merged.append((start, end))\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def _filter_intervals(self,\n",
    "                         intervals: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Filter out intervals shorter than minimum length.\n",
    "\n",
    "        Removes intervals that are too short to be considered meaningful\n",
    "        anomalies, reducing false positives from transient artifacts.\n",
    "\n",
    "        Args:\n",
    "            intervals: List of (start, end) interval tuples to filter.\n",
    "\n",
    "        Returns:\n",
    "            List of intervals meeting minimum length requirement.\n",
    "        \"\"\"\n",
    "        return [(start, end) for start, end in intervals\n",
    "                if end - start + 1 >= self.min_segment_length]\n",
    "\n",
    "    def detect_amplitude_anomalies(self, signal: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Detect amplitude-based anomalies.\n",
    "\n",
    "        Identifies samples with amplitudes significantly different from\n",
    "        the signal mean, using a z-score threshold approach.\n",
    "\n",
    "        Args:\n",
    "            signal: Preprocessed ECG signal array.\n",
    "\n",
    "        Returns:\n",
    "            Boolean mask where True indicates anomalous samples.\n",
    "        \"\"\"\n",
    "        mean_val = np.mean(signal)\n",
    "        std_val = np.std(signal)\n",
    "        return np.abs(signal - mean_val) > self.k_std * std_val\n",
    "\n",
    "    def detect_flatline_anomalies(self, signal: np.ndarray,\n",
    "                                 fs: int) -> np.ndarray:\n",
    "        \"\"\"Detect flatline anomalies.\n",
    "\n",
    "        Identifies regions with very low variability that may indicate\n",
    "        sensor disconnection or signal loss.\n",
    "\n",
    "        Args:\n",
    "            signal: Preprocessed ECG signal array.\n",
    "            fs: Sampling frequency in Hz.\n",
    "\n",
    "        Returns:\n",
    "            Boolean mask where True indicates potential flatline regions.\n",
    "        \"\"\"\n",
    "        window_size = int(self.flat_duration_sec * fs)\n",
    "        rolling_std = self._rolling_std(signal, window_size)\n",
    "        return rolling_std < 0.01  # Very low variability threshold\n",
    "\n",
    "    def detect_noise_anomalies(self, signal: np.ndarray,\n",
    "                              fs: int) -> np.ndarray:\n",
    "        \"\"\"Detect high-frequency noise anomalies.\n",
    "\n",
    "        Identifies regions with excessive high-frequency content that may\n",
    "        indicate muscle artifacts, electrical interference, or motion\n",
    "        artifacts.\n",
    "\n",
    "        Args:\n",
    "            signal: Preprocessed ECG signal array.\n",
    "            fs: Sampling frequency in Hz.\n",
    "\n",
    "        Returns:\n",
    "            Boolean mask where True indicates noisy regions.\n",
    "        \"\"\"\n",
    "        # Calculate signal derivative\n",
    "        diff_signal = np.diff(signal, prepend=signal[0])\n",
    "        global_std = np.std(diff_signal)\n",
    "\n",
    "        # Rolling standard deviation of derivative\n",
    "        window_size = int(self.noise_duration_sec * fs)\n",
    "        rolling_std = self._rolling_std(diff_signal, window_size)\n",
    "\n",
    "        return rolling_std > self.noise_multiplier * global_std\n",
    "\n",
    "    def detect_anomalies(self,\n",
    "                        signal: np.ndarray,\n",
    "                        fs: int,\n",
    "                        methods: List[str] = None) -> Dict:\n",
    "        \"\"\"Detect anomalies using specified methods and return intervals.\n",
    "\n",
    "        Applies selected detection methods to the signal and combines\n",
    "        results into a unified set of anomaly intervals.\n",
    "\n",
    "        Args:\n",
    "            signal: Preprocessed ECG signal array.\n",
    "            fs: Sampling frequency in Hz.\n",
    "            methods: List of detection methods to use. Options are\n",
    "                \"amplitude\", \"flatline\", \"noise\". If None, uses all methods.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - intervals: List of (start, end) anomaly intervals\n",
    "                - combined_mask: Boolean mask of all detected anomalies\n",
    "                - individual_masks: Dict of masks for each method\n",
    "                - methods_used: List of methods that were applied\n",
    "        \"\"\"\n",
    "        if methods is None:\n",
    "            methods = [\"amplitude\", \"flatline\", \"noise\"]\n",
    "\n",
    "        # Individual method masks\n",
    "        masks = {}\n",
    "\n",
    "        if \"amplitude\" in methods:\n",
    "            masks[\"amplitude\"] = self.detect_amplitude_anomalies(signal)\n",
    "\n",
    "        if \"flatline\" in methods:\n",
    "            masks[\"flatline\"] = self.detect_flatline_anomalies(signal, fs)\n",
    "\n",
    "        if \"noise\" in methods:\n",
    "            masks[\"noise\"] = self.detect_noise_anomalies(signal, fs)\n",
    "\n",
    "        # Combine all masks\n",
    "        if masks:\n",
    "            combined_mask = np.logical_or.reduce(list(masks.values()))\n",
    "        else:\n",
    "            combined_mask = np.zeros(len(signal), dtype=bool)\n",
    "\n",
    "        # Convert mask to intervals\n",
    "        intervals = self._mask_to_intervals(combined_mask)\n",
    "\n",
    "        # Filter and merge intervals\n",
    "        intervals = self._filter_intervals(intervals)\n",
    "        intervals = self._merge_intervals(intervals)\n",
    "\n",
    "        return {\n",
    "            \"intervals\": intervals,\n",
    "            \"combined_mask\": combined_mask,\n",
    "            \"individual_masks\": masks,\n",
    "            \"methods_used\": methods\n",
    "        }\n",
    "\n",
    "\n",
    "class GroundTruthLoader:\n",
    "    \"\"\"Load and process ground truth annotations.\n",
    "\n",
    "    This class handles loading and parsing of ground truth annotation files\n",
    "    for ECG anomaly detection evaluation. It supports filtering by quality\n",
    "    scores and handles various CSV formats.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_gt_intervals(annotation_file: str,\n",
    "                         quality_values: List[int] = None) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Load ground truth intervals from annotation CSV.\n",
    "\n",
    "        Reads annotation files containing ground truth anomaly intervals\n",
    "        and filters them based on quality scores.\n",
    "\n",
    "        Args:\n",
    "            annotation_file: Path to CSV file with annotations. Expected\n",
    "                format has intervals in columns 10-11 (1-indexed) and\n",
    "                quality scores in the last column.\n",
    "            quality_values: List of acceptable quality values for filtering.\n",
    "                If None, defaults to [2, 3] for high-quality annotations.\n",
    "\n",
    "        Returns:\n",
    "            List of (start, end) tuples for ground truth intervals,\n",
    "            sorted by start position. Returns empty list if file not found\n",
    "            or on error.\n",
    "        \"\"\"\n",
    "        if quality_values is None:\n",
    "            quality_values = [2, 3]\n",
    "\n",
    "        if not os.path.exists(annotation_file):\n",
    "            print(f\"Warning: Annotation file {annotation_file} not found\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Load CSV without header\n",
    "            df = pd.read_csv(annotation_file, header=None)\n",
    "\n",
    "            # Filter by quality (last column)\n",
    "            quality_col = df.columns[-1]\n",
    "            df_filtered = df[df[quality_col].isin(quality_values)]\n",
    "\n",
    "            # Extract intervals from columns 9 and 10 (0-indexed)\n",
    "            intervals = []\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                start = int(row[9])  # Column 10 (0-indexed)\n",
    "                end = int(row[10])   # Column 11 (0-indexed)\n",
    "\n",
    "                # Ensure valid interval\n",
    "                if start < end:\n",
    "                    intervals.append((start, end))\n",
    "\n",
    "            return sorted(intervals)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ground truth from {annotation_file}: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class AnomalyEvaluator:\n",
    "    \"\"\"Evaluate anomaly detection performance.\n",
    "\n",
    "    This class provides methods for evaluating the performance of anomaly\n",
    "    detection algorithms by comparing detected intervals with ground truth\n",
    "    annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_interval_overlap(interval1: Tuple[int, int],\n",
    "                                 interval2: Tuple[int, int]) -> float:\n",
    "        \"\"\"Calculate overlap ratio between two intervals.\n",
    "\n",
    "        Computes the Jaccard index (intersection over union) between two\n",
    "        intervals to measure their similarity.\n",
    "\n",
    "        Args:\n",
    "            interval1: First interval as (start, end) tuple.\n",
    "            interval2: Second interval as (start, end) tuple.\n",
    "\n",
    "        Returns:\n",
    "            Overlap ratio between 0 and 1, where 1 indicates perfect\n",
    "            overlap and 0 indicates no overlap.\n",
    "        \"\"\"\n",
    "        start1, end1 = interval1\n",
    "        start2, end2 = interval2\n",
    "\n",
    "        # Calculate intersection\n",
    "        intersection_start = max(start1, start2)\n",
    "        intersection_end = min(end1, end2)\n",
    "        intersection_length = max(0, intersection_end - intersection_start + 1)\n",
    "\n",
    "        # Calculate union\n",
    "        union_start = min(start1, start2)\n",
    "        union_end = max(end1, end2)\n",
    "        union_length = union_end - union_start + 1\n",
    "\n",
    "        return intersection_length / union_length if union_length > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_detection(detected_intervals: List[Tuple[int, int]],\n",
    "                          gt_intervals: List[Tuple[int, int]],\n",
    "                          overlap_threshold: float = 0.1) -> Dict:\n",
    "        \"\"\"Evaluate anomaly detection performance.\n",
    "\n",
    "        Compares detected anomaly intervals with ground truth intervals\n",
    "        and calculates standard evaluation metrics.\n",
    "\n",
    "        Args:\n",
    "            detected_intervals: List of detected anomaly intervals as\n",
    "                (start, end) tuples.\n",
    "            gt_intervals: List of ground truth intervals as (start, end)\n",
    "                tuples.\n",
    "            overlap_threshold: Minimum overlap ratio to consider a detection\n",
    "                as a true positive. Default 0.1 allows for slight\n",
    "                misalignment.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing evaluation metrics:\n",
    "                - true_positives: Number of correctly detected anomalies\n",
    "                - false_positives: Number of incorrect detections\n",
    "                - false_negatives: Number of missed ground truth intervals\n",
    "                - accuracy: Fraction of GT intervals detected\n",
    "                - precision: Fraction of detections that are correct\n",
    "                - recall: Fraction of GT intervals that were detected\n",
    "                - f1_score: Harmonic mean of precision and recall\n",
    "        \"\"\"\n",
    "        if not gt_intervals:\n",
    "            return {\n",
    "                \"true_positives\": 0,\n",
    "                \"false_positives\": len(detected_intervals),\n",
    "                \"false_negatives\": 0,\n",
    "                \"accuracy\": 0.0,\n",
    "                \"precision\": 0.0,\n",
    "                \"recall\": 0.0,\n",
    "                \"f1_score\": 0.0\n",
    "            }\n",
    "\n",
    "        matched_gt = set()\n",
    "        matched_detected = set()\n",
    "\n",
    "        # Find matches between detected and ground truth intervals\n",
    "        for i, detected in enumerate(detected_intervals):\n",
    "            for j, gt in enumerate(gt_intervals):\n",
    "                if j not in matched_gt:\n",
    "                    overlap = AnomalyEvaluator.calculate_interval_overlap(\n",
    "                        detected, gt)\n",
    "                    if overlap >= overlap_threshold:\n",
    "                        matched_detected.add(i)\n",
    "                        matched_gt.add(j)\n",
    "                        break\n",
    "\n",
    "        # Calculate metrics\n",
    "        true_positives = len(matched_gt)\n",
    "        false_positives = len(detected_intervals) - len(matched_detected)\n",
    "        false_negatives = len(gt_intervals) - len(matched_gt)\n",
    "\n",
    "        # Accuracy: fraction of GT intervals that have anomalies in them\n",
    "        accuracy = (true_positives / len(gt_intervals)\n",
    "                   if gt_intervals else 0.0)\n",
    "\n",
    "        # Standard precision, recall, F1\n",
    "        precision = (true_positives / len(detected_intervals)\n",
    "                    if detected_intervals else 0.0)\n",
    "        recall = (true_positives / len(gt_intervals)\n",
    "                 if gt_intervals else 0.0)\n",
    "        f1_score = (2 * precision * recall / (precision + recall)\n",
    "                   if (precision + recall) > 0 else 0.0)\n",
    "\n",
    "        return {\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }\n",
    "\n",
    "\n",
    "class AnomalyVisualizer:\n",
    "    \"\"\"Visualization utilities for anomaly detection.\n",
    "\n",
    "    This class provides methods for creating visualizations of ECG signals\n",
    "    with anomaly detection results, including comparison with ground truth\n",
    "    annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_anomalies(signal: np.ndarray,\n",
    "                      fs: int,\n",
    "                      detected_intervals: List[Tuple[int, int]],\n",
    "                      gt_intervals: List[Tuple[int, int]] = None,\n",
    "                      title: str = \"ECG Anomaly Detection\",\n",
    "                      time_unit: str = \"minutes\",\n",
    "                      figsize: Tuple[int, int] = (15, 6)) -> None:\n",
    "        \"\"\"Plot ECG signal with anomaly intervals.\n",
    "\n",
    "        Creates a visualization of the ECG signal with highlighted regions\n",
    "        showing detected anomalies and optionally ground truth intervals.\n",
    "\n",
    "        Args:\n",
    "            signal: ECG signal array to plot.\n",
    "            fs: Sampling frequency in Hz.\n",
    "            detected_intervals: List of detected anomaly intervals as\n",
    "                (start, end) tuples.\n",
    "            gt_intervals: Optional list of ground truth intervals for\n",
    "                comparison. If None, only detected intervals are shown.\n",
    "            title: Title for the plot.\n",
    "            time_unit: Time unit for x-axis, either \"seconds\" or \"minutes\".\n",
    "            figsize: Figure size as (width, height) tuple in inches.\n",
    "        \"\"\"\n",
    "        # Create time axis\n",
    "        time_samples = np.arange(len(signal))\n",
    "        if time_unit == \"minutes\":\n",
    "            time_axis = time_samples / (fs * 60)\n",
    "            time_label = \"Time (minutes)\"\n",
    "        else:\n",
    "            time_axis = time_samples / fs\n",
    "            time_label = \"Time (seconds)\"\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        # Plot ECG signal\n",
    "        plt.plot(time_axis, signal, \"k-\", linewidth=0.8, alpha=0.7,\n",
    "                label=\"ECG Signal\")\n",
    "\n",
    "        # Plot ground truth intervals\n",
    "        if gt_intervals:\n",
    "            for i, (start, end) in enumerate(gt_intervals):\n",
    "                start_time = (start / (fs * 60) if time_unit == \"minutes\"\n",
    "                             else start / fs)\n",
    "                end_time = (end / (fs * 60) if time_unit == \"minutes\"\n",
    "                           else end / fs)\n",
    "                plt.axvspan(start_time, end_time,\n",
    "                           color=\"blue\", alpha=0.2,\n",
    "                           label=\"Ground Truth\" if i == 0 else \"\")\n",
    "\n",
    "        # Plot detected intervals\n",
    "        for i, (start, end) in enumerate(detected_intervals):\n",
    "            start_time = (start / (fs * 60) if time_unit == \"minutes\"\n",
    "                         else start / fs)\n",
    "            end_time = (end / (fs * 60) if time_unit == \"minutes\"\n",
    "                       else end / fs)\n",
    "            plt.axvspan(start_time, end_time,\n",
    "                       color=\"red\", alpha=0.3,\n",
    "                       label=\"Detected Anomalies\" if i == 0 else \"\")\n",
    "\n",
    "        plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "        plt.xlabel(time_label, fontsize=12)\n",
    "        plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def print_summary(results: AnomalyResults, record_name: str) -> None:\n",
    "        \"\"\"Print summary statistics for anomaly detection.\n",
    "\n",
    "        Displays a formatted summary of detection results including\n",
    "        performance metrics and configuration details.\n",
    "\n",
    "        Args:\n",
    "            results: AnomalyResults object containing detection results.\n",
    "            record_name: Name of the analyzed record for display.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{\"=\"*60}\")\n",
    "        print(f\"ANOMALY DETECTION SUMMARY: {record_name}\")\n",
    "        print(f\"{\"=\"*60}\")\n",
    "        print(f\"Signal Length: {results.signal_length:,} samples\")\n",
    "        print(f\"Detection Methods: {\", \".join(results.detection_methods)}\")\n",
    "        print(f\"\\nGround Truth Intervals: {results.total_gt}\")\n",
    "        print(f\"Detected Intervals: {results.total_detected}\")\n",
    "        print(f\"True Positives: {results.true_positives}\")\n",
    "        print(f\"\\nAccuracy (GT intervals with anomalies): \"\n",
    "              f\"{results.accuracy:.3f}\")\n",
    "        print(f\"{\"=\"*60}\")\n",
    "\n",
    "\n",
    "def analyze_ecg_anomalies(data_dir: str,\n",
    "                         record_name: str,\n",
    "                         fs: int = 1000,\n",
    "                         detection_methods: List[str] = None,\n",
    "                         plot: bool = True,\n",
    "                         max_duration_hours: float | None = None,\n",
    "                         **detector_params) -> AnomalyResults:\n",
    "    \"\"\"Complete anomaly detection analysis for a single ECG record.\n",
    "\n",
    "    Performs end-to-end anomaly detection on an ECG record including\n",
    "    signal loading, preprocessing, anomaly detection, evaluation against\n",
    "    ground truth, and optional visualization.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files and annotations.\n",
    "        record_name: Record name without file extension (e.g., \"100001\").\n",
    "        fs: Expected sampling frequency in Hz. Will be overridden by\n",
    "            actual frequency from file.\n",
    "        detection_methods: List of detection methods to use. Options are\n",
    "            \"amplitude\", \"flatline\", \"noise\". If None, uses all methods.\n",
    "        plot: Whether to create a visualization of the results.\n",
    "        max_duration_hours: Maximum duration to process in hours. If None,\n",
    "            processes entire record.\n",
    "        **detector_params: Additional parameters passed to ECGAnomalyDetector\n",
    "            constructor (k_std, flat_duration_sec, etc.).\n",
    "\n",
    "    Returns:\n",
    "        AnomalyResults object containing complete analysis results including\n",
    "        detected intervals, ground truth comparison, and performance metrics.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If ECG file is not found.\n",
    "        ValueError: If signal processing fails.\n",
    "    \"\"\"\n",
    "    if detection_methods is None:\n",
    "        detection_methods = [\"amplitude\", \"flatline\", \"noise\"]\n",
    "\n",
    "    # Load and preprocess ECG signal\n",
    "    ecg_file = os.path.join(data_dir, f\"{record_name}_ECG\")\n",
    "    signal, actual_fs = load_ecg_file(ecg_file, max_duration_hours)\n",
    "    processed_signal = normalize_signal(signal)\n",
    "\n",
    "    # Use actual sampling frequency\n",
    "    fs = actual_fs\n",
    "\n",
    "    # Load ground truth annotations\n",
    "    annotation_file = os.path.join(data_dir, f\"{record_name}_ANN.csv\")\n",
    "    gt_loader = GroundTruthLoader()\n",
    "    gt_intervals = gt_loader.load_gt_intervals(annotation_file)\n",
    "\n",
    "    # Initialize detector and detect anomalies\n",
    "    detector = ECGAnomalyDetector(**detector_params)\n",
    "    detection_results = detector.detect_anomalies(processed_signal, fs,\n",
    "                                                 detection_methods)\n",
    "\n",
    "    # Evaluate detection performance\n",
    "    evaluator = AnomalyEvaluator()\n",
    "    eval_metrics = evaluator.evaluate_detection(\n",
    "        detection_results[\"intervals\"],\n",
    "        gt_intervals\n",
    "    )\n",
    "\n",
    "    # Create results object\n",
    "    results = AnomalyResults(\n",
    "        detected_intervals=detection_results[\"intervals\"],\n",
    "        gt_intervals=gt_intervals,\n",
    "        total_gt=len(gt_intervals),\n",
    "        total_detected=len(detection_results[\"intervals\"]),\n",
    "        true_positives=eval_metrics[\"true_positives\"],\n",
    "        accuracy=eval_metrics[\"accuracy\"],\n",
    "        detection_methods=detection_methods,\n",
    "        signal_length=len(processed_signal)\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    visualizer = AnomalyVisualizer()\n",
    "    visualizer.print_summary(results, record_name)\n",
    "\n",
    "    # Create visualization\n",
    "    if plot:\n",
    "        visualizer.plot_anomalies(\n",
    "            processed_signal,\n",
    "            fs,\n",
    "            detection_results[\"intervals\"],\n",
    "            gt_intervals,\n",
    "            title=f\"ECG Anomaly Detection - {record_name}\",\n",
    "            time_unit=\"minutes\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def batch_anomaly_analysis(data_dir: str,\n",
    "                          max_files: int | None = None,\n",
    "                          detection_methods: List[str] = None,\n",
    "                          max_duration_hours: float | None = None,\n",
    "                          **detector_params) -> pd.DataFrame:\n",
    "    \"\"\"Run anomaly detection on multiple ECG files.\n",
    "\n",
    "    Performs batch processing of ECG files in a directory, running\n",
    "    anomaly detection on each file and collecting results in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files and annotations.\n",
    "        max_files: Maximum number of files to process. If None, processes\n",
    "            all available files.\n",
    "        detection_methods: List of detection methods to use. If None,\n",
    "            uses all available methods.\n",
    "        max_duration_hours: Maximum duration per file in hours. If None,\n",
    "            processes entire files.\n",
    "        **detector_params: Additional parameters for ECGAnomalyDetector.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns:\n",
    "            - Record: Record name\n",
    "            - Total_GT_Intervals: Number of ground truth intervals\n",
    "            - Detected_Intervals: Number of detected intervals\n",
    "            - True_Positives: Number of correct detections\n",
    "            - Accuracy: Detection accuracy\n",
    "            - Signal_Length: Length of processed signal\n",
    "            - Methods: Detection methods used\n",
    "            - Error: Error message if processing failed\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If data directory is not found.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} not found!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Find ECG files\n",
    "    ecg_files = [f for f in os.listdir(data_dir) if f.endswith(\"_ECG.hea\")]\n",
    "    record_names = sorted([f.replace(\"_ECG.hea\", \"\") for f in ecg_files])\n",
    "\n",
    "    if max_files:\n",
    "        record_names = record_names[:max_files]\n",
    "\n",
    "    print(f\"Processing {len(record_names)} ECG files for anomaly detection...\")\n",
    "\n",
    "    results = []\n",
    "    for i, record_name in enumerate(record_names):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(record_names)}: {record_name}\")\n",
    "\n",
    "        try:\n",
    "            result = analyze_ecg_anomalies(\n",
    "                data_dir, record_name,\n",
    "                detection_methods=detection_methods,\n",
    "                plot=False,  # Don't plot for batch processing\n",
    "                max_duration_hours=max_duration_hours,\n",
    "                **detector_params\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"Record\": record_name,\n",
    "                \"Total_GT_Intervals\": result.total_gt,\n",
    "                \"Detected_Intervals\": result.total_detected,\n",
    "                \"True_Positives\": result.true_positives,\n",
    "                \"Accuracy\": result.accuracy,\n",
    "                \"Signal_Length\": result.signal_length,\n",
    "                \"Methods\": \", \".join(result.detection_methods)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {record_name}: {e}\")\n",
    "            results.append({\n",
    "                \"Record\": record_name,\n",
    "                \"Total_GT_Intervals\": 0,\n",
    "                \"Detected_Intervals\": 0,\n",
    "                \"True_Positives\": 0,\n",
    "                \"Accuracy\": 0.0,\n",
    "                \"Signal_Length\": 0,\n",
    "                \"Methods\": \"Error\",\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "\n",
    "        gc.collect()  # Memory cleanup\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if not df.empty and \"Accuracy\" in df.columns:\n",
    "        print(f\"\\n{\"=\"*60}\")\n",
    "        print(\"BATCH ANOMALY DETECTION SUMMARY\")\n",
    "        print(f\"{\"=\"*60}\")\n",
    "        print(f\"Files processed: {len(df)}\")\n",
    "        print(f\"Average accuracy: {df[\"Accuracy\"].mean():.3f}\")\n",
    "        print(f\"Total GT intervals: {df[\"Total_GT_Intervals\"].sum()}\")\n",
    "        print(f\"Total detected intervals: {df[\"Detected_Intervals\"].sum()}\")\n",
    "        print(f\"Total true positives: {df[\"True_Positives\"].sum()}\")\n",
    "        print(f\"{\"=\"*60}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_single_analysis():\n",
    "    \"\"\"Example of running single file analysis.\n",
    "\n",
    "    Demonstrates how to analyze a single ECG file with custom parameters\n",
    "    and visualize the results.\n",
    "\n",
    "    Returns:\n",
    "        AnomalyResults object with detection results.\n",
    "    \"\"\"\n",
    "    # Analyze single file\n",
    "    results = analyze_ecg_anomalies(\n",
    "        data_dir=\"source1\",\n",
    "        record_name=\"100001\",\n",
    "        detection_methods=[\"amplitude\", \"flatline\", \"noise\"],\n",
    "        k_std=2.048,\n",
    "        flat_duration_sec=1.043,\n",
    "        noise_duration_sec=1.134,\n",
    "        noise_multiplier=0.805,\n",
    "        min_segment_length=75,\n",
    "        merge_gap=89,\n",
    "        plot=True\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_batch_analysis():\n",
    "    \"\"\"Example of running batch analysis.\n",
    "\n",
    "    Demonstrates how to process multiple ECG files with consistent\n",
    "    parameters and collect results in a DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with batch analysis results.\n",
    "    \"\"\"\n",
    "    # Batch analysis\n",
    "    batch_results = batch_anomaly_analysis(\n",
    "        data_dir=\"source1\",\n",
    "        max_files=None,\n",
    "        detection_methods=[\"amplitude\", \"flatline\", \"noise\"],\n",
    "        max_duration_hours=None,\n",
    "        k_std=2.048,\n",
    "        flat_duration_sec=1.043,\n",
    "        noise_duration_sec=1.134,\n",
    "        noise_multiplier=0.805,\n",
    "        min_segment_length=75,\n",
    "        merge_gap=89\n",
    "    )\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649af853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  7. MAIN PIPELINE\n",
    "# =============================================================================\n",
    "def run_ecg_pipeline(data_dir1: str = \"source1\",\n",
    "                     data_dir2: str = \"source2\",\n",
    "                     max_duration_hours: float = 1.0,\n",
    "                     max_files: int = 5):\n",
    "    \"\"\"Run complete ECG analysis pipeline.\n",
    "\n",
    "    This function executes a comprehensive ECG analysis workflow including\n",
    "    single file analysis, batch processing, evaluation, and anomaly detection.\n",
    "    It processes ECG data from two directories with configurable limits on\n",
    "    file duration and number of files.\n",
    "\n",
    "    Args:\n",
    "        data_dir1 (str, optional): Directory path for general ECG analysis.\n",
    "            Should contain ECG files with \"_ECG.hea\" extension.\n",
    "            Defaults to \"source1\".\n",
    "        data_dir2 (str, optional): Directory path for evaluation containing\n",
    "            ECG files with annotations for R-peak detection validation.\n",
    "            Defaults to \"source2\".\n",
    "        max_duration_hours (float, optional): Maximum duration in hours to\n",
    "            process per ECG file. Files longer than this will be truncated.\n",
    "            Defaults to 1.0.\n",
    "        max_files (int, optional): Maximum number of files to process in\n",
    "            batch operations. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None: Function prints results to console and completes pipeline\n",
    "            execution without returning values.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If specified directories do not exist.\n",
    "        ValueError: If max_duration_hours or max_files are non-positive.\n",
    "\n",
    "    Example:\n",
    "        >>> run_ecg_pipeline(\"data/ecg\", \"data/annotations\", 0.5, 10)\n",
    "        === ECG Analysis Pipeline ===\n",
    "        Processing up to 0.5h per file, max 10 files\n",
    "        ...\n",
    "\n",
    "    Note:\n",
    "        - Requires ECG files with \"_ECG.hea\" extension in data_dir1\n",
    "        - Evaluation step requires annotated files in data_dir2\n",
    "        - Results are printed to console, not returned\n",
    "    \"\"\"\n",
    "    print(\"=== ECG Analysis Pipeline ===\")\n",
    "    print(f\"Processing up to {max_duration_hours}h per file, \"\n",
    "          f\"max {max_files} files\\n\")\n",
    "\n",
    "    # 1. Single file analysis\n",
    "    print(\"1. Single File Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    if os.path.exists(data_dir1):\n",
    "        files = [f.replace(\"_ECG.hea\", \"\") for f in os.listdir(data_dir1)\n",
    "                 if f.endswith(\"_ECG.hea\")]\n",
    "        if files:\n",
    "            sample_file = files[0]\n",
    "            analyze_ecg_file(data_dir1, sample_file, plot=True,\n",
    "                           max_duration_hours=max_duration_hours)\n",
    "\n",
    "    # 2. Batch processing\n",
    "    print(\"\\n2. Batch Processing\")\n",
    "    print(\"-\" * 30)\n",
    "    batch_analysis_results = analyze_ecg_batch(\n",
    "        data_dir1, max_files=max_files,\n",
    "        max_duration_hours=max_duration_hours)\n",
    "    if not batch_analysis_results.empty:\n",
    "        print(f\"\\nBatch Results Summary:\")\n",
    "        print(batch_analysis_results)\n",
    "\n",
    "    # 3. Evaluation\n",
    "    print(\"\\n3. R-Peak Detection Evaluation\")\n",
    "    print(\"-\" * 30)\n",
    "    eval_results = evaluate_batch(\n",
    "        data_dir2, max_files=max_files,\n",
    "        max_duration_hours=max_duration_hours)\n",
    "    if not eval_results.empty:\n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(eval_results)\n",
    "\n",
    "    # 4. Anomaly detection\n",
    "    print(\"\\n4. Anomaly Detection\")\n",
    "    print(\"-\" * 30)\n",
    "    anomaly_results = run_batch_analysis()\n",
    "\n",
    "    print(\"\\nPipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 18 ECG files for anomaly detection...\n",
      "\n",
      "Processing file 1/18: 100001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 100001\n",
      "============================================================\n",
      "Signal Length: 87,087,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 384\n",
      "Detected Intervals: 66402\n",
      "True Positives: 318\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.828\n",
      "============================================================\n",
      "\n",
      "Processing file 2/18: 100002\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 100002\n",
      "============================================================\n",
      "Signal Length: 86,762,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 114\n",
      "Detected Intervals: 61786\n",
      "True Positives: 93\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.816\n",
      "============================================================\n",
      "\n",
      "Processing file 3/18: 103001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 103001\n",
      "============================================================\n",
      "Signal Length: 87,125,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 94\n",
      "Detected Intervals: 27786\n",
      "True Positives: 2\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.021\n",
      "============================================================\n",
      "\n",
      "Processing file 4/18: 103002\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 103002\n",
      "============================================================\n",
      "Signal Length: 86,482,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 54\n",
      "Detected Intervals: 14747\n",
      "True Positives: 1\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.019\n",
      "============================================================\n",
      "\n",
      "Processing file 5/18: 103003\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 103003\n",
      "============================================================\n",
      "Signal Length: 86,420,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 86\n",
      "Detected Intervals: 30746\n",
      "True Positives: 35\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.407\n",
      "============================================================\n",
      "\n",
      "Processing file 6/18: 104001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 104001\n",
      "============================================================\n",
      "Signal Length: 87,178,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 51\n",
      "Detected Intervals: 22717\n",
      "True Positives: 37\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.725\n",
      "============================================================\n",
      "\n",
      "Processing file 7/18: 105001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 105001\n",
      "============================================================\n",
      "Signal Length: 139,147,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 678\n",
      "Detected Intervals: 8193\n",
      "True Positives: 123\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.181\n",
      "============================================================\n",
      "\n",
      "Processing file 8/18: 111001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 111001\n",
      "============================================================\n",
      "Signal Length: 90,645,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 1654\n",
      "Detected Intervals: 9385\n",
      "True Positives: 661\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.400\n",
      "============================================================\n",
      "\n",
      "Processing file 9/18: 113001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 113001\n",
      "============================================================\n",
      "Signal Length: 91,148,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 123\n",
      "Detected Intervals: 17428\n",
      "True Positives: 75\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.610\n",
      "============================================================\n",
      "\n",
      "Processing file 10/18: 114001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 114001\n",
      "============================================================\n",
      "Signal Length: 91,651,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 12\n",
      "Detected Intervals: 9772\n",
      "True Positives: 8\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.667\n",
      "============================================================\n",
      "\n",
      "Processing file 11/18: 115001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 115001\n",
      "============================================================\n",
      "Signal Length: 87,934,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 73\n",
      "Detected Intervals: 214\n",
      "True Positives: 0\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.000\n",
      "============================================================\n",
      "\n",
      "Processing file 12/18: 118001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 118001\n",
      "============================================================\n",
      "Signal Length: 89,091,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 61\n",
      "Detected Intervals: 36081\n",
      "True Positives: 44\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.721\n",
      "============================================================\n",
      "\n",
      "Processing file 13/18: 121001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 121001\n",
      "============================================================\n",
      "Signal Length: 91,233,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 85\n",
      "Detected Intervals: 42753\n",
      "True Positives: 51\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.600\n",
      "============================================================\n",
      "\n",
      "Processing file 14/18: 122001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 122001\n",
      "============================================================\n",
      "Signal Length: 122,798,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 13\n",
      "Detected Intervals: 1682\n",
      "True Positives: 0\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.000\n",
      "============================================================\n",
      "\n",
      "Processing file 15/18: 123001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 123001\n",
      "============================================================\n",
      "Signal Length: 133,448,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 58\n",
      "Detected Intervals: 97820\n",
      "True Positives: 51\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.879\n",
      "============================================================\n",
      "\n",
      "Processing file 16/18: 124001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 124001\n",
      "============================================================\n",
      "Signal Length: 86,489,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 363\n",
      "Detected Intervals: 39147\n",
      "True Positives: 145\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.399\n",
      "============================================================\n",
      "\n",
      "Processing file 17/18: 125001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 125001\n",
      "============================================================\n",
      "Signal Length: 86,428,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 19\n",
      "Detected Intervals: 60441\n",
      "True Positives: 8\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.421\n",
      "============================================================\n",
      "\n",
      "Processing file 18/18: 126001\n",
      "\n",
      "============================================================\n",
      "ANOMALY DETECTION SUMMARY: 126001\n",
      "============================================================\n",
      "Signal Length: 92,361,000 samples\n",
      "Detection Methods: amplitude, flatline, noise\n",
      "\n",
      "Ground Truth Intervals: 28\n",
      "Detected Intervals: 5133\n",
      "True Positives: 3\n",
      "\n",
      "Accuracy (GT intervals with anomalies): 0.107\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BATCH ANOMALY DETECTION SUMMARY\n",
      "============================================================\n",
      "Files processed: 18\n",
      "Average accuracy: 0.433\n",
      "Total GT intervals: 3950\n",
      "Total detected intervals: 552233\n",
      "Total true positives: 1655\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  8. USAGE\n",
    "# =============================================================================\n",
    "# Run the complete pipeline\n",
    "# run_ecg_pipeline()\n",
    "\n",
    "# Or run individual components:\n",
    "# analyze_ecg_file(\"source1\", \"100001\", plot=True)\n",
    "# batch_results = analyze_ecg_batch(\"source1\")\n",
    "# eval_results = evaluate_batch(\"source2\", max_files=5, max_duration_hours=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  9. PARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def optimize_anomaly_parameters_optuna(data_dir: str,\n",
    "                                     record_names: list = None,\n",
    "                                     max_files: int = 5,\n",
    "                                     max_duration_hours: float = 0.5,\n",
    "                                     optimization_metric: str = \"true_positives\",\n",
    "                                     n_trials: int = 100,\n",
    "                                     timeout: int = 3600,  # 1 hour timeout\n",
    "                                     verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Optimize ECG anomaly detection parameters using Optuna for efficient search.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing ECG files\n",
    "        record_names: Specific record names to use (if None, will auto-discover)\n",
    "        max_files: Maximum number of files to use for optimization\n",
    "        max_duration_hours: Maximum duration per file to process\n",
    "        optimization_metric: Metric to optimize (\"true_positives\", \"accuracy\", \"f1_score\", \"recall\")\n",
    "        n_trials: Maximum number of trials to run\n",
    "        timeout: Maximum time in seconds for optimization\n",
    "        verbose: Whether to print progress\n",
    "\n",
    "    Returns:\n",
    "        dict: Best parameters and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Get record names if not provided\n",
    "    if record_names is None:\n",
    "        import os\n",
    "        if not os.path.exists(data_dir):\n",
    "            print(f\"Error: Directory {data_dir} not found!\")\n",
    "            return None\n",
    "\n",
    "        ecg_files = [f for f in os.listdir(data_dir) if f.endswith(\"_ECG.hea\")]\n",
    "        record_names = sorted([f.replace(\"_ECG.hea\", \"\") for f in ecg_files])\n",
    "\n",
    "    # Limit number of files for optimization\n",
    "    record_names = record_names[:max_files]\n",
    "\n",
    "    if not record_names:\n",
    "        print(\"No ECG files found!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Optimizing parameters using {len(record_names)} files:\")\n",
    "    print(f\"Files: {record_names}\")\n",
    "    print(f\"Using Optuna with {n_trials} trials and {timeout}s timeout\")\n",
    "    print(f\"Optimizing for: {optimization_metric}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function to maximize.\"\"\"\n",
    "\n",
    "        # Sample parameters\n",
    "        k_std = trial.suggest_float(\"k_std\", 0.8, 3.0)\n",
    "        flat_duration_sec = trial.suggest_float(\"flat_duration_sec\", 0.5, 4.0)\n",
    "        noise_duration_sec = trial.suggest_float(\"noise_duration_sec\", 0.2, 1.5)\n",
    "        noise_multiplier = trial.suggest_float(\"noise_multiplier\", 0.5, 3.0)\n",
    "        min_segment_length = trial.suggest_int(\"min_segment_length\", 10, 150)\n",
    "        merge_gap = trial.suggest_int(\"merge_gap\", 10, 150)\n",
    "\n",
    "        # Sample detection methods (categorical with multiple selection)\n",
    "        use_amplitude = trial.suggest_categorical(\"use_amplitude\", [True, False])\n",
    "        use_flatline = trial.suggest_categorical(\"use_flatline\", [True, False])\n",
    "        use_noise = trial.suggest_categorical(\"use_noise\", [True, False])\n",
    "\n",
    "        # Ensure at least one method is selected\n",
    "        methods = []\n",
    "        if use_amplitude:\n",
    "            methods.append(\"amplitude\")\n",
    "        if use_flatline:\n",
    "            methods.append(\"flatline\")\n",
    "        if use_noise:\n",
    "            methods.append(\"noise\")\n",
    "\n",
    "        if not methods:\n",
    "            methods = [\"amplitude\"]  # Default fallback\n",
    "\n",
    "        # Create parameter dict\n",
    "        params = {\n",
    "            \"k_std\": k_std,\n",
    "            \"flat_duration_sec\": flat_duration_sec,\n",
    "            \"noise_duration_sec\": noise_duration_sec,\n",
    "            \"noise_multiplier\": noise_multiplier,\n",
    "            \"min_segment_length\": min_segment_length,\n",
    "            \"merge_gap\": merge_gap\n",
    "        }\n",
    "\n",
    "        # Test this parameter combination across all files\n",
    "        total_true_positives = 0\n",
    "        total_gt_intervals = 0\n",
    "        total_detected = 0\n",
    "        total_accuracy = 0\n",
    "        successful_files = 0\n",
    "\n",
    "        for record_name in record_names:\n",
    "            try:\n",
    "                # Run anomaly detection with current parameters\n",
    "                result = analyze_ecg_anomalies(\n",
    "                    data_dir=data_dir,\n",
    "                    record_name=record_name,\n",
    "                    detection_methods=methods,\n",
    "                    plot=False,\n",
    "                    max_duration_hours=max_duration_hours,\n",
    "                    **params\n",
    "                )\n",
    "\n",
    "                total_true_positives += result.true_positives\n",
    "                total_gt_intervals += result.total_gt\n",
    "                total_detected += result.total_detected\n",
    "                total_accuracy += result.accuracy\n",
    "                successful_files += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                # Penalize failed trials\n",
    "                continue\n",
    "\n",
    "        if successful_files == 0:\n",
    "            return 0  # Return 0 for failed trials\n",
    "\n",
    "        # Calculate aggregate metrics\n",
    "        overall_accuracy = total_true_positives / total_gt_intervals if total_gt_intervals > 0 else 0\n",
    "        precision = total_true_positives / total_detected if total_detected > 0 else 0\n",
    "        recall = total_true_positives / total_gt_intervals if total_gt_intervals > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Store intermediate values for analysis\n",
    "        trial.set_user_attr(\"total_true_positives\", total_true_positives)\n",
    "        trial.set_user_attr(\"total_gt_intervals\", total_gt_intervals)\n",
    "        trial.set_user_attr(\"total_detected\", total_detected)\n",
    "        trial.set_user_attr(\"overall_accuracy\", overall_accuracy)\n",
    "        trial.set_user_attr(\"precision\", precision)\n",
    "        trial.set_user_attr(\"recall\", recall)\n",
    "        trial.set_user_attr(\"f1_score\", f1_score)\n",
    "        trial.set_user_attr(\"successful_files\", successful_files)\n",
    "        trial.set_user_attr(\"methods\", methods)\n",
    "\n",
    "        # Return optimization metric\n",
    "        if optimization_metric == \"true_positives\":\n",
    "            return total_true_positives\n",
    "        elif optimization_metric == \"accuracy\":\n",
    "            return overall_accuracy\n",
    "        elif optimization_metric == \"f1_score\":\n",
    "            return f1_score\n",
    "        elif optimization_metric == \"recall\":\n",
    "            return recall\n",
    "        elif optimization_metric == \"precision\":\n",
    "            return precision\n",
    "        else:\n",
    "            return total_true_positives  # Default\n",
    "\n",
    "    # Create and run study\n",
    "    sampler = TPESampler(seed=42)  # Reproducible results\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=sampler,\n",
    "        study_name=\"ecg_anomaly_optimization\"\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout,\n",
    "        show_progress_bar=verbose\n",
    "    )\n",
    "\n",
    "    # Get best results\n",
    "    best_trial = study.best_trial\n",
    "    best_params = {k: v for k, v in best_trial.params.items()\n",
    "                  if not k.startswith(\"use_\")}\n",
    "\n",
    "    # Reconstruct best methods\n",
    "    best_methods = []\n",
    "    if best_trial.params.get(\"use_amplitude\", False):\n",
    "        best_methods.append(\"amplitude\")\n",
    "    if best_trial.params.get(\"use_flatline\", False):\n",
    "        best_methods.append(\"flatline\")\n",
    "    if best_trial.params.get(\"use_noise\", False):\n",
    "        best_methods.append(\"noise\")\n",
    "\n",
    "    if not best_methods:\n",
    "        best_methods = [\"amplitude\"]\n",
    "\n",
    "    # Create results dataframe\n",
    "    trials_df = study.trials_dataframe()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTUNA OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best {optimization_metric}: {study.best_value:.3f}\")\n",
    "    print(f\"Best trial number: {best_trial.number}\")\n",
    "    print(f\"Total trials completed: {len(study.trials)}\")\n",
    "    print(f\"Best methods: {best_methods}\")\n",
    "    print(f\"Best parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value:.3f}\" if isinstance(value, float) else f\"  {param}: {value}\")\n",
    "\n",
    "    # Show performance metrics for best trial\n",
    "    best_attrs = best_trial.user_attrs\n",
    "    print(f\"\\nBest trial performance:\")\n",
    "    print(f\"  True Positives: {best_attrs.get(\"total_true_positives\", \"N/A\")}\")\n",
    "    print(f\"  Accuracy: {best_attrs.get(\"overall_accuracy\", 0):.3f}\")\n",
    "    print(f\"  Precision: {best_attrs.get(\"precision\", 0):.3f}\")\n",
    "    print(f\"  Recall: {best_attrs.get(\"recall\", 0):.3f}\")\n",
    "    print(f\"  F1 Score: {best_attrs.get(\"f1_score\", 0):.3f}\")\n",
    "\n",
    "    # Show top 5 trials\n",
    "    top_trials = sorted(study.trials, key=lambda t: t.value or 0, reverse=True)[:5]\n",
    "    print(f\"\\nTop 5 trials:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, trial in enumerate(top_trials):\n",
    "        if trial.value is not None:\n",
    "            methods_str = trial.user_attrs.get(\"methods\", [])\n",
    "            print(f\"{i+1}. Score: {trial.value:.3f} | Trial: {trial.number} | \"\n",
    "                  f\"Methods: {methods_str} | TP: {trial.user_attrs.get(\"total_true_positives\", 0)}\")\n",
    "\n",
    "    return {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_methods\": best_methods,\n",
    "        \"best_score\": study.best_value,\n",
    "        \"best_trial\": best_trial,\n",
    "        \"optimization_metric\": optimization_metric,\n",
    "        \"study\": study,\n",
    "        \"trials_df\": trials_df,\n",
    "        \"summary\": {\n",
    "            \"total_trials\": len(study.trials),\n",
    "            \"files_used\": record_names,\n",
    "            \"best_true_positives\": best_attrs.get(\"total_true_positives\", 0),\n",
    "            \"best_accuracy\": best_attrs.get(\"overall_accuracy\", 0),\n",
    "            \"best_precision\": best_attrs.get(\"precision\", 0),\n",
    "            \"best_recall\": best_attrs.get(\"recall\", 0),\n",
    "            \"best_f1_score\": best_attrs.get(\"f1_score\", 0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def quick_optuna_optimization(data_dir: str,\n",
    "                            max_files: int = 3,\n",
    "                            max_duration_hours: float = 0.25,\n",
    "                            n_trials: int = 50):\n",
    "    \"\"\"Quick Optuna optimization with fewer trials.\"\"\"\n",
    "    print(\"Running QUICK Optuna optimization...\")\n",
    "    print(f\"Using {n_trials} trials for faster results\")\n",
    "\n",
    "    return optimize_anomaly_parameters_optuna(\n",
    "        data_dir=data_dir,\n",
    "        max_files=max_files,\n",
    "        max_duration_hours=max_duration_hours,\n",
    "        optimization_metric=\"true_positives\",\n",
    "        n_trials=n_trials,\n",
    "        timeout=1800,  # 30 minutes\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def comprehensive_optuna_optimization(data_dir: str,\n",
    "                                    max_files: int = 5,\n",
    "                                    max_duration_hours: float = 0.5,\n",
    "                                    n_trials: int = 200):\n",
    "    \"\"\"Comprehensive Optuna optimization with more trials.\"\"\"\n",
    "    print(\"Running COMPREHENSIVE Optuna optimization...\")\n",
    "    print(f\"Using {n_trials} trials for thorough search\")\n",
    "\n",
    "    return optimize_anomaly_parameters_optuna(\n",
    "        data_dir=data_dir,\n",
    "        max_files=max_files,\n",
    "        max_duration_hours=max_duration_hours,\n",
    "        optimization_metric=\"true_positives\",\n",
    "        n_trials=n_trials,\n",
    "        timeout=36000,  # 2 hours\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def multi_objective_optimization(data_dir: str,\n",
    "                               max_files: int = 5,\n",
    "                               max_duration_hours: float = 0.5,\n",
    "                               n_trials: int = 150):\n",
    "    \"\"\"\n",
    "    Run optimization for multiple objectives and compare results.\n",
    "    \"\"\"\n",
    "    print(\"Running MULTI-OBJECTIVE optimization...\")\n",
    "\n",
    "    objectives = [\"true_positives\", \"accuracy\", \"f1_score\", \"recall\"]\n",
    "    results = {}\n",
    "\n",
    "    for objective in objectives:\n",
    "        print(f\"\\nOptimizing for: {objective}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        result = optimize_anomaly_parameters_optuna(\n",
    "            data_dir=data_dir,\n",
    "            max_files=max_files,\n",
    "            max_duration_hours=max_duration_hours,\n",
    "            optimization_metric=objective,\n",
    "            n_trials=n_trials,\n",
    "            timeout=1800,  # 30 minutes per objective\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        results[objective] = result\n",
    "\n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-OBJECTIVE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_data = []\n",
    "    for obj, result in results.items():\n",
    "        if result:\n",
    "            comparison_data.append({\n",
    "                \"Objective\": obj,\n",
    "                \"Best_Score\": result[\"best_score\"],\n",
    "                \"True_Positives\": result[\"summary\"][\"best_true_positives\"],\n",
    "                \"Accuracy\": result[\"summary\"][\"best_accuracy\"],\n",
    "                \"Precision\": result[\"summary\"][\"best_precision\"],\n",
    "                \"Recall\": result[\"summary\"][\"best_recall\"],\n",
    "                \"F1_Score\": result[\"summary\"][\"best_f1_score\"],\n",
    "                \"Methods\": result[\"best_methods\"]\n",
    "            })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "def run_quick_optuna():\n",
    "    \"\"\"Run quick Optuna optimization\"\"\"\n",
    "    return quick_optuna_optimization(\n",
    "        data_dir=\"source1\",  # Update with your data directory\n",
    "        max_files=3,\n",
    "        max_duration_hours=0.25,\n",
    "        n_trials=50\n",
    "    )\n",
    "\n",
    "def run_comprehensive_optuna():\n",
    "    \"\"\"Run comprehensive Optuna optimization\"\"\"\n",
    "    return comprehensive_optuna_optimization(\n",
    "        data_dir=\"source1\",  # Update with your data directory\n",
    "        max_files=14,\n",
    "        max_duration_hours=0.5,\n",
    "        n_trials=50000\n",
    "    )\n",
    "\n",
    "def run_multi_objective():\n",
    "    \"\"\"Run multi-objective optimization\"\"\"\n",
    "    return multi_objective_optimization(\n",
    "        data_dir=\"source1\",  # Update with your data directory\n",
    "        max_files=4,\n",
    "        max_duration_hours=0.4,\n",
    "        n_trials=100\n",
    "    )\n",
    "\n",
    "# Run quick optimization by default\n",
    "print(\"Running comprehensive optimization...\")\n",
    "optimal_results = run_comprehensive_optuna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bba98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMAL PARAMETERS FOR YOUR FUNCTIONS:\n",
      "============================================================\n",
      "Use these parameters in your analyze_ecg_anomalies() function:\n",
      "\n",
      "# Optimal detection methods:\n",
      "detection_methods = ['amplitude', 'flatline', 'noise']\n",
      "\n",
      "# Optimal parameters:\n",
      "k_std = 2.048\n",
      "flat_duration_sec = 1.043\n",
      "noise_duration_sec = 1.134\n",
      "noise_multiplier = 0.805\n",
      "min_segment_length = 75\n",
      "merge_gap = 89\n",
      "\n",
      "# Example function call:\n",
      "results = analyze_ecg_anomalies(\n",
      "    data_dir=\"your_data_directory\",\n",
      "    record_name=\"your_record_name\",\n",
      "    detection_methods=['amplitude', 'flatline', 'noise'],\n",
      "    k_std=2.048,\n",
      "    flat_duration_sec=1.043,\n",
      "    noise_duration_sec=1.134,\n",
      "    noise_multiplier=0.805,\n",
      "    min_segment_length=75,\n",
      "    merge_gap=89,\n",
      "    plot=True\n",
      ")\n",
      "\n",
      "This configuration achieved:\n",
      "- True Positives: 27\n",
      "- Accuracy: 0.008\n",
      "- Precision: 0.003\n",
      "- Recall: 0.008\n",
      "- F1 Score: 0.004\n",
      "- Total trials: 4875\n",
      "\n",
      "Optuna insights:\n",
      "- Best trial number: 295\n",
      "- Optimization took 4875 trials\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  10. ANOMALY DETECTION SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "if optimal_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMAL PARAMETERS FOR YOUR FUNCTIONS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Use these parameters in your analyze_ecg_anomalies() function:\")\n",
    "    print()\n",
    "\n",
    "    # Format parameters for easy copy-paste\n",
    "    params = optimal_results[\"best_params\"]\n",
    "    methods = optimal_results[\"best_methods\"]\n",
    "\n",
    "    print(\"# Optimal detection methods:\")\n",
    "    print(f\"detection_methods = {methods}\")\n",
    "    print()\n",
    "    print(\"# Optimal parameters:\")\n",
    "    for param, value in params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{param} = {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{param} = {value}\")\n",
    "\n",
    "    print()\n",
    "    print(\"# Example function call:\")\n",
    "    print(\"results = analyze_ecg_anomalies(\")\n",
    "    print(\"    data_dir=\"your_data_directory\",\")\n",
    "    print(\"    record_name=\"your_record_name\",\")\n",
    "    print(f\"    detection_methods={methods},\")\n",
    "    for param, value in params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"    {param}={value:.3f},\")\n",
    "        else:\n",
    "            print(f\"    {param}={value},\")\n",
    "    print(\"    plot=True\")\n",
    "    print(\")\")\n",
    "\n",
    "    print(f\"\\nThis configuration achieved:\")\n",
    "    summary = optimal_results[\"summary\"]\n",
    "    print(f\"- True Positives: {summary[\"best_true_positives\"]}\")\n",
    "    print(f\"- Accuracy: {summary[\"best_accuracy\"]:.3f}\")\n",
    "    print(f\"- Precision: {summary[\"best_precision\"]:.3f}\")\n",
    "    print(f\"- Recall: {summary[\"best_recall\"]:.3f}\")\n",
    "    print(f\"- F1 Score: {summary[\"best_f1_score\"]:.3f}\")\n",
    "    print(f\"- Total trials: {summary[\"total_trials\"]}\")\n",
    "\n",
    "    # Optuna study insights\n",
    "    print(f\"\\nOptuna insights:\")\n",
    "    print(f\"- Best trial number: {optimal_results[\"best_trial\"].number}\")\n",
    "    print(f\"- Optimization took {summary[\"total_trials\"]} trials\")\n",
    "\n",
    "else:\n",
    "    print(\"Optimization failed. Please check your data directory and files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
